import requests
import json
import base64
import re
import concurrent.futures
import os
import time
from urllib.parse import quote, urlparse

# ================= 1. é…ç½®åŒºåŸŸ =================

# ã€Token è®¾ç½®ã€‘æœ¬åœ°è¿è¡Œå¯å¡«ï¼ŒGitHub Actions ç•™ç©º
MY_GITHUB_TOKEN = "" 

# ã€ä»£ç†è®¾ç½®ã€‘Mac æœ¬åœ°å»ºè®®å¡« None æˆ–å…·ä½“çš„ Clash åœ°å€
PROXIES = None 
# PROXIES = {"http": "http://127.0.0.1:7890", "https": "http://127.0.0.1:7890"}

# ã€æºåˆ—è¡¨ã€‘åŒ…å«å•ä»“å’Œå¤šä»“ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨è¯†åˆ«å¤„ç†
SOURCE_URLS = [
    # --- å•ä»“ ---
    "http://www.é¥­å¤ªç¡¬.com/tv",
    "http://è‚¥çŒ«.com",
    "http://fty.xxooo.cf/tv",
    "https://æ¯’ç›’.com/tv/",
    "http://æˆ‘ä¸æ˜¯.æ‘¸é±¼å„¿.com",
    "http://ok321.top/tv",
    "http://ok321.top/ok",
    "http://tvbox.ç‹äºŒå°æ”¾ç‰›å¨ƒ.top",
    "https://www.yingm.cc/dm/dm.json",
    "http://home.jundie.top:81/top98.json",
    "http://cdn.qiaoji8.com/tvbox.json",
    "https://cdn.gitmirror.com/bb/xduo/libs/master/index.json",
    "https://gitee.com/free-kingdom/dc/raw/main/T4.json",
    "https://raw.githubusercontent.com/yoursmile66/TVBox/main/XC.json",
    "https://tv.èœå¦®ä¸.top",
    "https://api.hgyx.vip/hgyx.json",
    "https://dxawi.github.io/0/0.json",
    "http://www.mitvbox.xyz/å°ç±³/DEMO.json",
    "http://xhztv.top/xhz",
    "http://xhztv.top/4k.json",
    "https://9877.kstore.space/AnotherD/api.json",
    "https://raw.githubusercontent.com/xyq254245/xyqonlinerule/main/XYQTVBox.json",
    "https://bitbucket.org/xduo/duoapi/raw/master/xpg.json",
    "https://raw.githubusercontent.com/guot55/YGBH/main/vip2.json",
    "http://tv.nxog.top/m/111.php?ou=å…¬ä¼—å·æ¬§æ­Œapp&mz=all&jar=all&b=æ¬§æ­Œ",
    "https://å“ªå’.live/",
    "https://www.252035.xyz/z/FongMi.json",
    "http://www.meowtv.vip/tvbox.json",
    "http://fmys.top/fmys.json",
    "https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json",
    "https://gitee.com/yiwu369/6758/raw/master/%E9%9D%92%E9%BE%99/1.json",
    "https://raw.githubusercontent.com/maoystv/6/main/000.json",
    "https://cnb.cool/fish2018/duanju/-/git/raw/main/tvbox.json",
    "https://raw.githubusercontent.com/chitue/dongliTV/main/api.json",
    "https://cnb.cool/aooooowuuuuu/FreeSpider/-/git/raw/main/config",
    "https://android.lushunming.qzz.io/json/index.json",
    
    # --- å¤šä»“ (è„šæœ¬ä¼šè‡ªåŠ¨å±•å¼€) ---
    "https://www.iyouhun.com/tv/dc",
    "https://www.iyouhun.com/tv/yh",
    "https://9877.kstore.space/AnotherDS/api.json",
    "http://xhztv.top/dc/",
    "http://xhztv.top/DC.txt",
    "https://bitbucket.org/xduo/cool/raw/main/room.json",
    "https://qixing.myhkw.com/DC.txt",
    "http://xmbjm.fh4u.org/dc.txt"
]

# ä¼˜åŒ–é…ç½®
ENABLE_GITHUB_SEARCH = True   # å¼€å¯è‡ªåŠ¨æœå¯»
MAX_GITHUB_RESULTS = 5
TIMEOUT = 4                   # é€‚åº¦æ”¾å®½è¶…æ—¶ï¼Œä¿è¯æŠ“å–ç‡
BLACKLIST = ["å¤±æ•ˆ", "æµ‹è¯•", "å¹¿å‘Š", "æ”¶è´¹", "ç¾¤", "åŠ V", "æŒ‚å£", "Qç¾¤", "ä¼¦ç†", "ç¦åˆ©", "æˆäºº"]

# ================= 2. åŸºç¡€å·¥å…·å‡½æ•° =================

def decode_content(content):
    """è§£å¯† TVBox å„ç§å¥‡è‘©æ ¼å¼"""
    if not content: return None
    content = content.strip()
    try:
        return json.loads(content)
    except:
        pass
    try:
        # ç®€å•å¤„ç†å¹²æ‰°å­—ç¬¦å’ŒBase64
        clean_content = content.replace('**', '').replace('o', '').strip() if content.startswith('**') else content
        decoded = base64.b64decode(clean_content).decode('utf-8')
        return json.loads(decoded)
    except:
        pass
    try:
        match = re.search(r'\{.*\}', content, re.DOTALL)
        if match:
            return json.loads(match.group())
    except:
        pass
    return None

def get_json(url):
    """å¸¦é‡è¯•çš„ç½‘ç»œè¯·æ±‚"""
    safe_url = quote(url, safe=':/?&=')
    headers = {"User-Agent": "Mozilla/5.0", "Referer": safe_url}
    try:
        res = requests.get(url, headers=headers, timeout=6, verify=False, proxies=PROXIES)
        res.encoding = 'utf-8'
        if res.status_code == 200:
            return decode_content(res.text)
    except:
        pass
    return None

def fetch_github_sources():
    """GitHub è‡ªåŠ¨æœå¯»"""
    print(">>> [1/5] æ­£åœ¨è¿æ¥ GitHub æ¢ç´¢æ–°æº...")
    token = os.getenv("GH_TOKEN") or MY_GITHUB_TOKEN
    
    if "ghp_" not in token:
        print("    [!] æœªé…ç½®æœ‰æ•ˆ Tokenï¼Œè·³è¿‡ GitHub æœç´¢ã€‚")
        return []
        
    urls = []
    headers = {"Authorization": f"token {token}", "Accept": "application/vnd.github.v3+json"}
    api = "https://api.github.com/search/code?q=filename:json+spider+sites+tvbox&sort=indexed&order=desc"
    
    try:
        r = requests.get(api, headers=headers, timeout=10, verify=False, proxies=PROXIES)
        if r.status_code == 200:
            items = r.json().get('items', [])
            print(f"    [+] GitHub å‘ç° {len(items)} ä¸ªæ½œåœ¨æºæ–‡ä»¶")
            for item in items[:MAX_GITHUB_RESULTS]:
                raw = item.get('html_url', '').replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')
                if raw: urls.append(raw)
    except Exception as e:
        print(f"    [!] GitHub æœç´¢å‡ºé”™: {e}")
    return urls

# ================= 3. æ ¸å¿ƒé€»è¾‘ï¼šå¤šä»“å±•å¼€ä¸èåˆ =================

def clean_name(name):
    """åç§°æ¸…æ´—"""
    name = re.sub(r'ã€.*?ã€‘|\[.*?\]|\(.*?\)', '', name)
    name = name.replace("èšåˆ", "").replace("è“å…‰", "").replace("ä¸“çº¿", "").replace("API", "").strip()
    return name if name else "æœªå‘½åæ¥å£"

def expand_multirepo(urls):
    """ã€æ–°åŠŸèƒ½ã€‘é€’å½’å±•å¼€å¤šä»“åˆ—è¡¨"""
    print(f"\n>>> [2/5] æ­£åœ¨è§£æ {len(urls)} ä¸ªåˆå§‹åœ°å€ (æ™ºèƒ½è¯†åˆ«å•ä»“/å¤šä»“)...")
    
    final_single_repos = []
    
    def check_url(url):
        data = get_json(url)
        if not data: return None
        
        # æƒ…å†µA: æ˜¯å¤šä»“ (åŒ…å« urls åˆ—è¡¨)
        if 'urls' in data and isinstance(data['urls'], list):
            print(f"    [+] å‘ç°å¤šä»“: {url} -> åŒ…å« {len(data['urls'])} ä¸ªå­æº")
            sub_urls = []
            for item in data['urls']:
                if isinstance(item, dict) and 'url' in item:
                    sub_urls.append(item['url'])
            return ("MULTI", sub_urls)
            
        # æƒ…å†µB: æ˜¯å•ä»“ (åŒ…å« sites åˆ—è¡¨)
        elif 'sites' in data:
            # print(f"    [.] ç¡®è®¤å•ä»“: {url}")
            return ("SINGLE", url)
            
        return None

    # å¹¶å‘é¢„æ£€æŸ¥
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        future_to_url = {executor.submit(check_url, url): url for url in urls}
        for future in concurrent.futures.as_completed(future_to_url):
            res = future.result()
            if res:
                rtype, content = res
                if rtype == "SINGLE":
                    final_single_repos.append(content)
                elif rtype == "MULTI":
                    # å°†å¤šä»“é‡Œçš„å­é“¾æ¥ç›´æ¥åŠ å…¥å¾…å¤„ç†åˆ—è¡¨
                    final_single_repos.extend(content)

    # å»é‡
    final_single_repos = list(set(final_single_repos))
    print(f"    -> æœ€ç»ˆè§£æå‡º {len(final_single_repos)} ä¸ªæœ‰æ•ˆçš„å•ä»“é…ç½®åœ°å€ã€‚")
    return final_single_repos

def test_site_latency(site):
    """æµ‹é€Ÿ + éªŒè¯"""
    name = site.get('name', '')
    api = site.get('api', '')
    
    for kw in BLACKLIST:
        if kw in name: return None
        
    # åªå–é€šç”¨ CMS (0/1) å’Œ APP (4)
    if site.get('type') not in [0, 1, 4]:
        return None

    headers = {"User-Agent": "Mozilla/5.0"}
    start_time = time.time()
    
    try:
        r = requests.get(api, headers=headers, timeout=TIMEOUT, stream=True, verify=False, proxies=PROXIES)
        if r.status_code < 400:
            latency = (time.time() - start_time) * 1000
            site['_latency'] = int(latency)
            site['name'] = clean_name(name)
            
            if latency < 800: site['name'] = f"ğŸš€ {site['name']}"
            elif latency < 1500: site['name'] = f"ğŸŸ¢ {site['name']}"
            else: site['name'] = f"ğŸŸ¡ {site['name']}"
            
            # print(f"    [âˆš] {int(latency)}ms | {site['name']}")
            return site
    except:
        pass
    return None

def main():
    requests.packages.urllib3.disable_warnings()
    print(">>> å¯åŠ¨ TVBox å…¨ç½‘èåˆè„šæœ¬ v6.0 (å¤šä»“ç‰ˆ)")
    
    # 1. å‡†å¤‡åˆå§‹åˆ—è¡¨
    initial_urls = SOURCE_URLS.copy()
    if ENABLE_GITHUB_SEARCH:
        initial_urls.extend(fetch_github_sources())
        
    # 2. å±•å¼€å¤šä»“ï¼Œè·å–æ‰€æœ‰å•ä»“åœ°å€
    all_config_urls = expand_multirepo(initial_urls)
    
    # 3. æ‰«ææå–æ¥å£
    print(f"\n>>> [3/5] æ­£åœ¨æ·±åº¦æ‰«æ {len(all_config_urls)} ä¸ªé…ç½®...")
    
    skeleton_config = None
    raw_sites = []
    
    # è¿™é‡Œä¸éœ€è¦å¤ªé«˜å¹¶å‘ï¼Œä»¥å…è¢«æºç«™å°IP
    for i, url in enumerate(all_config_urls):
        # ç®€å•çš„è¿›åº¦æ˜¾ç¤º
        # print(f"    å¤„ç† ({i+1}/{len(all_config_urls)}): {url}")
        data = get_json(url)
        if not data: continue
        
        # æŠ“å–éª¨æ¶ (ä¼˜å…ˆæ‰¾å¸¦ jar çš„)
        if not skeleton_config and data.get('spider'):
            skeleton_config = {
                "spider": data.get('spider'),
                "wallpaper": data.get('wallpaper'),
                "lives": data.get('lives', []), 
                "parses": data.get('parses', []),
                "flags": data.get('flags', [])
            }
            # ä¿ç•™ä¸»æºçš„ Spider æ¥å£
            for s in data.get('sites', []):
                if s.get('type') == 3:
                    s['name'] = f"â˜… {clean_name(s['name'])}"
                    s['_latency'] = 0
                    raw_sites.append(s)

        # æå– CMS æ¥å£
        for s in data.get('sites', []):
            if s.get('type') in [0, 1, 4]:
                raw_sites.append(s)

    if not skeleton_config:
        print("\n[!!!] æ‚²å‰§ï¼šæ‰€æœ‰æºé‡Œéƒ½æ²¡æ‰¾åˆ°ä¸€ä¸ªå¯ç”¨çš„ Spider/Jarï¼Œæ— æ³•ç”Ÿæˆæœ‰æ•ˆé…ç½®ã€‚")
        # ç´§æ€¥ä¿åº•ï¼ˆé˜²æ­¢ç©ºæ–‡ä»¶ï¼‰ï¼šéšä¾¿é€ ä¸€ä¸ªéª¨æ¶
        skeleton_config = {"spider": "", "sites": [], "lives": []}

    # 4. å»é‡ä¸æµ‹é€Ÿ
    print(f"\n>>> [4/5] æ­£åœ¨ç«é€Ÿæ¸…æ´— (åŸå§‹æ¥å£: {len(raw_sites)} ä¸ª)...")
    
    unique_sites = {}
    tasks = []
    
    for s in raw_sites:
        api = s.get('api')
        if api:
            if s.get('type') == 3:
                unique_sites[api] = s # Spideræ¥å£ç›´æ¥ä¿ç•™
            elif api not in unique_sites: # é¿å…é‡å¤æµ‹åŒä¸€ä¸ªAPI
                tasks.append(s) 

    valid_sites = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor: # æé«˜å¹¶å‘åŠ é€Ÿæµ‹é€Ÿ
        futures = [executor.submit(test_site_latency, site) for site in tasks]
        for future in concurrent.futures.as_completed(futures):
            res = future.result()
            if res:
                if res['api'] not in unique_sites:
                    unique_sites[res['api']] = res
                    valid_sites.append(res)

    # 5. æ’åºä¸è¾“å‡º
    print(f"\n>>> [5/5] æ­£åœ¨ç”Ÿæˆæœ€ç»ˆåˆ—è¡¨...")
    
    vip_sites = [s for s in unique_sites.values() if s.get('_latency') == 0]
    common_sites = sorted(valid_sites, key=lambda x: x['_latency']) # æŒ‰å»¶è¿Ÿæ’åº
    
    final_sites = vip_sites + common_sites
    
    # æ¸…ç†å†…éƒ¨å­—æ®µ
    for s in final_sites:
        s.pop('_latency', None)

    skeleton_config['sites'] = final_sites
    
    with open("my_tvbox.json", 'w', encoding='utf-8') as f:
        json.dump(skeleton_config, f, ensure_ascii=False, indent=2)

    print(f"\n" + "="*40)
    print(f"âœ… å…¨ç½‘èåˆå®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ï¼š")
    print(f"   - åˆå§‹åœ°å€æ•°: {len(initial_urls)}")
    print(f"   - è§£æå•ä»“æ•°: {len(all_config_urls)} (å«è‡ªåŠ¨è£‚å˜)")
    print(f"   - åŸå§‹æ¥å£æ± : {len(raw_sites)}")
    print(f"   - æœ€ç»ˆæœ‰æ•ˆæº: {len(final_sites)}")
    print(f"   - ğŸš€ æé€Ÿæº:   {len([s for s in valid_sites if 'ğŸš€' in s['name']])} ä¸ª")
    print(f"ğŸ“‚ æ–‡ä»¶è·¯å¾„: {os.path.abspath('my_tvbox.json')}")
    print(f"="*40)

if __name__ == "__main__":
    main()