import requests
import json
import re
import concurrent.futures
import os
import sys
from urllib.parse import quote, urljoin

# ================= 1. é…ç½®åŒºåŸŸ =================

MY_GITHUB_TOKEN = "" 
PROXIES = None 

# ã€å…¨å±€é…ç½®ã€‘
GITHUB_USER = "guru2016"
REPO_NAME = "tvbox-pro"
BRANCH_NAME = "main"
# å…¨å±€ä¿åº• Jar (ä½ çš„é˜²æ¯’ç›¾ç‰Œ)
GLOBAL_SAFE_JAR = f"https://cdn.jsdelivr.net/gh/{GITHUB_USER}/{REPO_NAME}@{BRANCH_NAME}/spider.jar"

# ã€æ ¸å¿ƒï¼šæºä¸Jarçš„æ™ºèƒ½åŒ¹é…è¡¨ã€‘
# æ ¼å¼: "è®¢é˜…åœ°å€": "è¯¥åœ°å€å¼ºåˆ¶ä½¿ç”¨çš„Jar"
JAR_MAP = {
    # --- ä¿¡ä»»çš„å¤§å‚ï¼šä¿ç•™åŸé… (å…¼å®¹æ€§ä¼˜å…ˆ) ---
    "http://www.é¥­å¤ªç¡¬.com/tv": "http://www.é¥­å¤ªç¡¬.com/To/jar/3.jar",
    "http://è‚¥çŒ«.com": "http://è‚¥çŒ«.com/è‚¥çŒ«.jar",
    "http://fty.xxooo.cf/tv": "http://www.é¥­å¤ªç¡¬.com/To/jar/3.jar",
    "http://rihou.cc:88/è·åŸèŒ¶ç§€": "http://rihou.cc:88/jar/è·åŸèŒ¶ç§€.jar",
    
    # --- âš  éœ€å‡€åŒ–çš„å¤§å‚ï¼šå¼ºåˆ¶ä½¿ç”¨ä½ çš„çº¯å‡€Jar (å»å¹¿å‘Šä¼˜å…ˆ) ---
    "http://cdn.qiaoji8.com/tvbox.json": GLOBAL_SAFE_JAR,
    "http://tvbox.ç‹äºŒå°æ”¾ç‰›å¨ƒ.top": GLOBAL_SAFE_JAR,  # ç»å¸¸å˜åŠ¨ï¼Œç”¨çº¯å‡€Jaræ›´ç¨³
}

# ã€æœåˆ®åˆ—è¡¨ã€‘(ç²¾é€‰40+ä¼˜è´¨æº)
EXTERNAL_URLS = list(JAR_MAP.keys()) + [
    # --- ç¨³å®šå¤§å‚ ---
    "https://raw.githubusercontent.com/yoursmile66/TVBox/main/XC.json", # å—é£
    "https://raw.githubusercontent.com/guot55/YGBH/main/vip2.json",     # å®ç›’
    "https://cnb.cool/fish2018/duanju/-/git/raw/main/tvbox.json",       # çŸ­å‰§
    "https://raw.githubusercontent.com/chitue/dongliTV/main/api.json",  # åŠ¨åŠ›
    "https://cdn.gitmirror.com/bb/xduo/libs/master/index.json",          # é“é•¿
    
    # --- ä¼˜è´¨å•ä»“ ---
    "http://ok321.top/tv",
    "https://tv.èœå¦®ä¸.top",
    "https://api.hgyx.vip/hgyx.json",
    "https://android.lushunming.qzz.io/json/index.json",
    "http://home.jundie.top:81/top98.json",
    "https://ghproxy.net/https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json",
    "https://www.252035.xyz/z/FongMi.json",
    "http://52bsj.vip:81/api/v3/file/get/29899/bsj2023.json?sign=3c594b2b985b365bad", # è¿è¾“è½¦
    
    # --- æ½œåŠ›æ–°æº ---
    "https://s2.pub/x",
    "http://tv.nxog.top/m/111.php?ou=å…¬ä¼—å·æ¬§æ­Œapp&mz=all&jar=all&b=æ¬§æ­Œ",
    "https://100km.top/0",
    "https://tvbox.cainisi.cf",
    "http://meowtv.cn/tv",
    "https://weixine.net/ysc.json",
    "http://8.210.232.168/xc.json",
    "https://cdn.jsdelivr.net/gh/2hacc/TVBox@main/tvbox.json",
    "https://raw.githubusercontent.com/undCover/PyramidStore/main/pyramid.json",
    "http://dxawi.github.io/0/0.json",
    "https://raw.githubusercontent.com/chengxueli818913/maoTV/main/44.json",
    "https://agit.ai/Yoursmile7/TVBox/raw/branch/master/XC.json"
]

# ã€è¿‡æ»¤é…ç½®ã€‘
ALLOWED_TYPES = [0, 1, 3, 4] 
# å¹¿å‘Šæ‹¦æˆªå…³é”®è¯
BLACKLIST = [
    "å¤±æ•ˆ", "æµ‹è¯•", "å¹¿å‘Š", "æ”¶è´¹", "ç¾¤", "åŠ V", "æŒ‚å£", "Qç¾¤", "ä¼¦ç†", "ç¦åˆ©", "æˆäºº", "æƒ…è‰²", 
    "å¼•æµ", "å¼¹å¹•", "æ›´æ–°", "å…¬ä¼—å·", "æ‰«ç ", "å¾®ä¿¡", "ä¼é¹…", "APP", "ä¸‹è½½", 
    "æ¨å¹¿", "éªŒè¯", "æ¿€æ´»", "æˆæƒ", "é›·é²¸", "ç©å¶å“¥å“¥", "åŠ©æ‰‹", "ä¸“çº¿", "å½©è›‹", "ç›´æ’­",
    "77.110", "mingming"
]

# ã€æ€§èƒ½é…ç½®ã€‘
TIMEOUT = 6        # å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´ (ç§’)
MAX_WORKERS = 60   # æé€Ÿå¹¶å‘æ•° (GitHub Actions æ€§èƒ½è¶³å¤Ÿæ”¯æŒ)

# ================= 2. å·¥å…·å‡½æ•° =================

def decode_content(content):
    if not content: return None
    try: return json.loads(content)
    except: pass
    try:
        clean = content.replace('**', '').replace('o', '').strip()
        return json.loads(base64.b64decode(clean).decode('utf-8'))
    except:
        try:
            match = re.search(r'\{.*\}', content, re.DOTALL)
            if match: return json.loads(match.group())
        except: pass
    return None

def get_json(url):
    """æ™®é€šè·å–ï¼Œç”¨äºæ‹‰å–é…ç½®åˆ—è¡¨"""
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, timeout=TIMEOUT, verify=False, proxies=PROXIES)
        res.encoding = 'utf-8'
        if res.status_code == 200:
            return decode_content(res.text)
    except: pass
    return None

def clean_name(name):
    name = str(name)
    name = re.sub(r'ã€.*?ã€‘|\[.*?\]|\(.*?\)|ï¼ˆ.*?ï¼‰', '', name)
    name = name.replace("èšåˆ", "").replace("è“å…‰", "").replace("ä¸“çº¿", "").strip()
    return name

# ================= 3. æ ¸å¿ƒï¼šå®šå‘ Jar æ³¨å…¥ =================

def fetch_and_inject_jar(url):
    data = get_json(url)
    if not data: return []
    
    extracted_sites = []
    # æŸ¥æ‰¾æ˜¯å¦æœ‰é¢„è®¾çš„ Jar
    target_jar = JAR_MAP.get(url) 
    
    def process_site(site):
        # åªæœ‰ Type 3 éœ€è¦ Jar
        if site.get('type') == 3:
            # ç­–ç•¥ A: å‘½ä¸­ JAR_MAP (å¦‚é¥­å¤ªç¡¬æˆ–å·§æŠ€)
            if target_jar:
                site['jar'] = target_jar # å¼ºåˆ¶ä½¿ç”¨æˆ‘ä»¬æŒ‡å®šçš„ (åŸé…æˆ–çº¯å‡€)
            
            # ç­–ç•¥ B: æ•£æˆ·æº
            elif 'jar' in site:
                # å†æ¬¡æ£€æŸ¥ Jar æ˜¯å¦åŒ…å«æ¯’ç˜¤åŸŸå
                jar_url = str(site['jar'])
                if "qiaoji" in jar_url or "mingming" in jar_url:
                    site['jar'] = GLOBAL_SAFE_JAR # æ€æ¯’
                else:
                    pass # æš‚æ—¶ä¿¡ä»»åŸé…
            else:
                site['jar'] = GLOBAL_SAFE_JAR # æ— Jaråˆ™è¡¥å…¨
        return site

    # æå–å¤šä»“
    if 'urls' in data and isinstance(data['urls'], list):
        for item in data['urls']:
            if 'url' in item:
                sub_data = get_json(item['url'])
                if sub_data and 'sites' in sub_data:
                    for s in sub_data['sites']:
                        extracted_sites.append(process_site(s))
    
    # æå–å•ä»“
    if 'sites' in data:
        for s in data['sites']:
            extracted_sites.append(process_site(s))
            
    return extracted_sites

# ================= 4. æµç¨‹å‡½æ•° =================

def fetch_all_sites_with_jars():
    print(f">>> [1/4] æé€Ÿæœåˆ® (å¹¶å‘: {MAX_WORKERS})...")
    if "guru2016" not in GLOBAL_SAFE_JAR:
        print("!!! è­¦å‘Šï¼šGITHUB_USER æœªé…ç½®ï¼")
        
    all_sites = []
    # ä½¿ç”¨ set å»é‡ URLï¼Œé˜²æ­¢é‡å¤çˆ¬å–
    unique_urls = list(set(EXTERNAL_URLS))
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
        future_to_url = {executor.submit(fetch_and_inject_jar, url): url for url in unique_urls}
        for future in concurrent.futures.as_completed(future_to_url):
            try:
                sites = future.result()
                if sites: all_sites.extend(sites)
            except: pass
            
    print(f"    [+] åŸå§‹æ¥å£æ•°é‡: {len(all_sites)}")
    return all_sites

def validate_and_filter(sites):
    print(f">>> [2/4] å¾®åˆ›æµ‹é€Ÿ & æ·±åº¦æ¸…æ´—...")
    valid_sites = []
    seen_api = set()
    tasks = []
    
    for s in sites:
        name = s.get('name', '')
        api = s.get('api', '')
        stype = s.get('type', 0)
        
        if stype not in ALLOWED_TYPES: continue
        if any(bw in name for bw in BLACKLIST): continue
        if api in seen_api: continue
        # æ’é™¤ emoji å¹¿å‘Š
        if any(char in name for char in ['ğŸ’°', 'ğŸ‘—', 'ğŸ‘ ', 'âœ¨', 'âš¡', 'ğŸ”¥', 'å…è´¹', 'é€', 'åŠ V']): continue

        seen_api.add(api)
        tasks.append(s)

    # âš¡ï¸ æé€Ÿæ£€æµ‹å‡½æ•° âš¡ï¸
    def fast_check(site):
        try:
            # å…³é”®ä¼˜åŒ–ï¼šstream=Trueï¼Œåªè¯»å‰512å­—èŠ‚
            # åªè¦èƒ½å»ºç«‹è¿æ¥ä¸”è¿”å›å°‘é‡æ•°æ®ï¼Œå°±è§†ä¸ºå­˜æ´»ï¼Œæå¤§å‡å°‘è€—æ—¶
            with requests.get(site['api'], timeout=TIMEOUT, stream=True, verify=False, proxies=PROXIES) as res:
                if res.status_code == 200:
                    # å°è¯•è¯»å–ä¸€ç‚¹ç‚¹æ•°æ®ï¼Œç¡®ä¿ä¸æ˜¯ç©ºè¿æ¥
                    chunk = next(res.iter_content(chunk_size=512), None)
                    if chunk:
                        latency = int(res.elapsed.total_seconds() * 1000)
                        site['_latency'] = latency
                        site['name'] = clean_name(site['name'])
                        site['searchable'] = 1 
                        site['quickSearch'] = 1
                        
                        # å›¾æ ‡é€»è¾‘
                        site_jar = site.get('jar', '')
                        if site_jar == GLOBAL_SAFE_JAR:
                            site['name'] = f"ğŸ›¡ï¸ {site['name']}" # å‡€åŒ–è¿‡çš„
                        elif site_jar:
                            site['name'] = f"ğŸ§© {site['name']}" # åŸé…Jar
                        else:
                            site['name'] = f"ğŸš€ {site['name']}" # CMS/App
                            
                        return site
        except: pass
        return None

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(fast_check, s) for s in tasks]
        for future in concurrent.futures.as_completed(futures):
            res = future.result()
            if res: valid_sites.append(res)
            
    # æ’åº
    valid_sites.sort(key=lambda x: x['_latency'])
    for s in valid_sites: s.pop('_latency', None)
    
    print(f"    [âˆš] å­˜æ´»æ¥å£: {len(valid_sites)} ä¸ª")
    return valid_sites

def main():
    try:
        requests.packages.urllib3.disable_warnings()
        print(">>> å¯åŠ¨ TVBox æé€Ÿä¸‡æºç‰ˆ v24.0")
        
        raw_sites = fetch_all_sites_with_jars()
        final_sites = validate_and_filter(raw_sites)
        
        # æ•°é‡é™åˆ¶æ”¾å®½åˆ° 200ï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨æœ‰è¶³å¤Ÿå¤šçš„å¥½æº
        max_sites = 200
        if len(final_sites) > max_sites:
            final_sites = final_sites[:max_sites]
        
        config = {
            "spider": GLOBAL_SAFE_JAR,
            "wallpaper": "https://api.kdcc.cn",
            "sites": final_sites,
            "lives": [],
            "parses": [],
            "flags": []
        }
        
        print(f">>> [3/4] ç”Ÿæˆé…ç½®...")
        with open("my_tvbox.json", 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
            
        print(f"\nâœ… å®Œæˆï¼")
        print(f"ğŸ“Š æœ€ç»ˆæ”¶å½•: {len(final_sites)} ä¸ª")
        print(f"ğŸ›¡ï¸ å‡€åŒ–æº(å·§æŠ€ç­‰): {len([s for s in final_sites if 'ğŸ›¡ï¸' in s['name']])} ä¸ª")
        print(f"ğŸ§© åŸé…æº(é¥­/è‚¥ç­‰): {len([s for s in final_sites if 'ğŸ§©' in s['name']])} ä¸ª")
        
    except Exception as e:
        print(f"\n[!!!] é”™è¯¯: {e}")
        if not os.path.exists("my_tvbox.json"):
            with open("my_tvbox.json", 'w', encoding='utf-8') as f:
                json.dump({"spider":"", "sites":[]}, f)
        sys.exit(0)

if __name__ == "__main__":
    main()
