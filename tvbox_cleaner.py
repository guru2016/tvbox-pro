import requests
import json
import re
import concurrent.futures
import os
import sys
import base64
from urllib.parse import quote, urljoin

# ================= 1. é…ç½®åŒºåŸŸ =================

# ã€å…¨å±€å”¯ä¸€ Jarï¼šç”¨æˆ·æŒ‡å®š GitHub ç›´è¿ã€‘
# æ³¨æ„ï¼šå›½å†…ç½‘ç»œç›´æ¥è®¿é—®æ­¤é“¾æ¥å¯èƒ½ä¼šæ…¢æˆ–å¤±è´¥ï¼Œä½†åœ¨ TVBox å†…éƒ¨é€šå¸¸èƒ½è‡ªåŠ¨å¤„ç† 302 è·³è½¬
GLOBAL_SAFE_JAR = "https://github.com/guru2016/tvbox-pro/raw/refs/heads/main/custom_spider.jar"

# ã€å£çº¸ã€‘(æ›¿æ¢é“é•¿ä¸ç¨³å®šçš„å£çº¸)
WALLPAPER_URL = "https://api.kdcc.cn"

# ã€åº•æ¿æ¥æºï¼šé“é•¿ dr_py å®˜æ–¹é…ç½®ã€‘
# æˆ‘ä»¬å°†ä»¥è¿™ä¸ªæ–‡ä»¶çš„ç»“æ„ï¼ˆparses, rules, flagsï¼‰ä¸ºåŸºç¡€è¿›è¡Œä¿®æ”¹
BASE_CONFIG_URL = "https://raw.githubusercontent.com/hjdhnx/dr_py/main/tvbox.json"

# ã€è¿½åŠ æœåˆ®åˆ—è¡¨ã€‘
# åœ¨é“é•¿çš„åŸºç¡€ä¸Šï¼Œæ·»åŠ è¿™äº›ä¼˜è´¨æº
ADDITIONAL_URLS = [
    "http://www.é¥­å¤ªç¡¬.com/tv",
    "http://è‚¥çŒ«.com",
    "https://raw.githubusercontent.com/yoursmile66/TVBox/main/XC.json",      # å—é£
    "https://raw.githubusercontent.com/guot55/YGBH/main/vip2.json",          # å®ç›’
    "https://raw.githubusercontent.com/chitue/dongliTV/main/api.json",       # åŠ¨åŠ›
    "https://cdn.gitmirror.com/bb/xduo/libs/master/index.json",              # é“é•¿é•œåƒ
    "https://api.hgyx.vip/hgyx.json",
    "https://tv.èœå¦®ä¸.top"
]

# ã€è¿‡æ»¤é…ç½®ã€‘
ALLOWED_TYPES = [0, 1, 3, 4] 

# ã€é€šç”¨é»‘åå•ã€‘
BLACKLIST = [
    "å¤±æ•ˆ", "æµ‹è¯•", "å¹¿å‘Š", "æ”¶è´¹", "ç¾¤", "åŠ V", "æŒ‚å£", "Qç¾¤", "ä¼¦ç†", "ç¦åˆ©", "æˆäºº", "æƒ…è‰²", 
    "å¼•æµ", "æ›´æ–°", "æ‰«ç ", "å¾®ä¿¡", "ä¼é¹…", "APP", "ä¸‹è½½", "æ¨å¹¿", "éªŒè¯", "æ¿€æ´»", "æˆæƒ", 
    "é›·é²¸", "ç©å¶å“¥å“¥", "åŠ©æ‰‹", "ä¸“çº¿", "å½©è›‹", "ç›´æ’­", "77.110", "mingming", "æ‘¸é±¼"
]

# ã€ç½‘ç›˜ç‰¹å¾è¯ã€‘(ç”¨äºæ¸…æ´—é“é•¿åŸæ¥çš„ç½‘ç›˜æ¥å£)
# é‡åˆ°è¿™äº›è¯ï¼Œç›´æ¥æ€æ‰
DISK_KEYWORDS = [
    "é˜¿é‡Œäº‘", "å¤¸å…‹", "UCç½‘ç›˜", "115", "ç½‘ç›˜", "äº‘ç›˜", "æ¨é€", "å­˜å‚¨", 
    "Drive", "Ali", "Quark", "Alist", "1359527.xyz" # å±è”½é“é•¿ç§æœ‰æœåŠ¡å™¨
]

TIMEOUT = 20       
MAX_WORKERS = 40   

# ================= 2. å·¥å…·å‡½æ•° =================

def decode_content(content):
    if not content: return None
    try: return json.loads(content)
    except: pass
    try:
        clean = content.replace('**', '').replace('o', '').strip()
        return json.loads(base64.b64decode(clean).decode('utf-8'))
    except:
        try:
            match = re.search(r'\{.*\}', content, re.DOTALL)
            if match: return json.loads(match.group())
        except: pass
    return None

def get_json(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        # verify=False å¿½ç•¥è¯ä¹¦é”™è¯¯
        res = requests.get(url, headers=headers, timeout=TIMEOUT, verify=False)
        res.encoding = 'utf-8'
        if res.status_code == 200:
            return decode_content(res.text)
    except: pass
    return None

def clean_name(name):
    name = str(name)
    name = re.sub(r'ã€.*?ã€‘|\[.*?\]|\(.*?\)|ï¼ˆ.*?ï¼‰', '', name)
    name = name.replace("èšåˆ", "").replace("è“å…‰", "").replace("ä¸“çº¿", "").strip()
    return name

# ================= 3. æ ¸å¿ƒå¤„ç†é€»è¾‘ =================

def process_site(site):
    """
    æ¸…æ´—å•ä¸ª Site å¯¹è±¡
    """
    # 1. å¼ºåˆ¶å‰¥ç¦» Jar
    if 'jar' in site:
        del site['jar']
        
    name = site.get('name', '')
    api = str(site.get('api', ''))
    
    # 2. ç½‘ç›˜ & Alist è¿‡æ»¤
    is_disk = False
    if any(k in name for k in DISK_KEYWORDS): is_disk = True
    if not is_disk:
        api_lower = api.lower()
        if any(k.lower() in api_lower for k in DISK_KEYWORDS): is_disk = True
        
    if is_disk:
        return None

    # 3. å¹¿å‘Šè¿‡æ»¤
    if any(bw in name for bw in BLACKLIST): return None
    
    # 4. æ ‡è®°ä¸ç¾åŒ–
    site['name'] = clean_name(name)
    site['searchable'] = 1 
    site['quickSearch'] = 1
    
    # 5. ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœ Type 3 æ¥å£çš„ API çœ‹èµ·æ¥æ˜¯éœ€è¦é“é•¿ç§æœ‰æœåŠ¡å™¨çš„(æ¯”å¦‚ drpy.min.js)ï¼Œ
    # å› ä¸ºæˆ‘ä»¬æ¢äº† Jarï¼Œè¿™äº›å¤§æ¦‚ç‡ä¼šå´©ã€‚å»ºè®®ä¿ç•™æ ‡å‡† CMS (Type 0/1) å’Œå…¼å®¹æ€§å¥½çš„ Type 3ã€‚
    # è¿™é‡Œæˆ‘ä»¬åªä¿ç•™åå­—é‡Œå¸¦ "drpy" ä½† API ä¹Ÿæ˜¯ http çš„ï¼Œæˆ–è€…æ ‡å‡†çš„ CSPã€‚
    
    if site.get('type') == 3:
        site['name'] = f"ğŸ›¡ï¸ {site['name']}" 
    else:
        site['name'] = f"ğŸš€ {site['name']}" 
        
    return site

def fetch_sites_from_url(url):
    """
    ä»æŒ‡å®š URL æŠ“å–å¹¶æ¸…æ´— sites
    """
    print(f"    -> æŠ“å–æ‰©å±•æº: {url}")
    data = get_json(url)
    if not data: return []
    
    extracted = []
    
    # å¤„ç†å¤šä»“
    if 'urls' in data and isinstance(data['urls'], list):
        for item in data['urls']:
            if 'url' in item:
                sub = get_json(item['url'])
                if sub and 'sites' in sub:
                    for s in sub['sites']:
                        p = process_site(s)
                        if p: extracted.append(p)
    
    # å¤„ç†å•ä»“
    if 'sites' in data:
        for s in data['sites']:
            p = process_site(s)
            if p: extracted.append(p)
            
    return extracted

def main():
    try:
        requests.packages.urllib3.disable_warnings()
        print(">>> å¯åŠ¨ TVBox v41.0 (é“é•¿åº•æ¿+ç§æœ‰Jar+èåˆç‰ˆ)")
        
        # 1. è·å–é“é•¿åº•æ¿é…ç½®
        print(f">>> [1/3] ä¸‹è½½é“é•¿åº•æ¿é…ç½®: {BASE_CONFIG_URL}")
        base_config = get_json(BASE_CONFIG_URL)
        
        if not base_config:
            print("!!! æ— æ³•ä¸‹è½½åº•æ¿ï¼Œå°†ä½¿ç”¨ç©ºæ¨¡æ¿")
            base_config = {"spider": "", "sites": [], "parses": [], "flags": [], "rules": []}
            
        # 2. ä¿®æ”¹åº•æ¿æ ¸å¿ƒå‚æ•°
        base_config['spider'] = GLOBAL_SAFE_JAR   # æ›¿æ¢ Jar
        base_config['wallpaper'] = WALLPAPER_URL  # æ›¿æ¢å£çº¸
        base_config['drives'] = []                # æ¸…ç©ºç½‘ç›˜æŒ‚è½½ (æ ¸å¿ƒå»ç½‘ç›˜æ­¥éª¤)
        
        # 3. æ¸…æ´—é“é•¿åŸæœ‰çš„ Sites
        # é“é•¿çš„æºé‡Œæ··åˆäº†å¤§é‡ç½‘ç›˜ï¼Œéœ€è¦æ¸…æ´—
        print(">>> [2/3] æ¸…æ´—é“é•¿åŸæœ‰æ¥å£...")
        clean_base_sites = []
        if 'sites' in base_config:
            for s in base_config['sites']:
                processed = process_site(s)
                if processed:
                    clean_base_sites.append(processed)
        
        # 4. å¹¶å‘æŠ“å–è¿½åŠ æº
        print(f">>> [3/3] èåˆå…¶ä»– {len(ADDITIONAL_URLS)} ä¸ªå¤§å‚æº...")
        additional_sites = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_url = {executor.submit(fetch_sites_from_url, url): url for url in ADDITIONAL_URLS}
            for future in concurrent.futures.as_completed(future_to_url):
                try:
                    sites = future.result()
                    if sites: additional_sites.extend(sites)
                except: pass
        
        # 5. åˆå¹¶ä¸å»é‡
        # é¡ºåºï¼šé“é•¿æ¸…æ´—åçš„æº + è¿½åŠ çš„å¤§å‚æº
        all_sites = clean_base_sites + additional_sites
        unique_sites = []
        seen_api = set()
        
        for s in all_sites:
            api = s.get('api', '')
            if api and api not in seen_api:
                unique_sites.append(s)
                seen_api.add(api)
        
        # æˆªæ–­
        if len(unique_sites) > 350:
            unique_sites = unique_sites[:350]
            
        base_config['sites'] = unique_sites
        
        # 6. ä¿å­˜
        with open("my_tvbox.json", 'w', encoding='utf-8') as f:
            json.dump(base_config, f, ensure_ascii=False, indent=2)
            
        print(f"\nâœ… å®Œæˆï¼")
        print(f"ğŸ“Š æœ€ç»ˆæ¥å£: {len(unique_sites)} ä¸ª")
        print(f"ğŸ§¬ ç»§æ‰¿: é“é•¿ Parses/Rules/Flags")
        print(f"ğŸ§¹ å‰”é™¤: é“é•¿ Drives (ç½‘ç›˜æŒ‚è½½)")
        print(f"ğŸ›¡ï¸ æ ¸å¿ƒ Jar: {GLOBAL_SAFE_JAR}")
        
    except Exception as e:
        print(f"\n[!!!] é”™è¯¯: {e}")
        if not os.path.exists("my_tvbox.json"):
            with open("my_tvbox.json", 'w', encoding='utf-8') as f:
                json.dump({"spider":GLOBAL_SAFE_JAR, "sites":[]}, f)
        sys.exit(0)

if __name__ == "__main__":
    main()
