import requests
import json
import re
import concurrent.futures
import os
import sys
import base64
from urllib.parse import quote, urljoin

# ================= 1. é…ç½®åŒºåŸŸ =================

# ã€å…¨å±€å”¯ä¸€ Jarï¼šé¥­å¤ªç¡¬å®˜æ–¹ç›´è¿ã€‘
# å…¼å®¹æ€§ä¹‹ç‹ï¼Œèƒ½é©±åŠ¨ç»å¤§å¤šæ•°æ¥å£
GLOBAL_SAFE_JAR = "http://www.é¥­å¤ªç¡¬.com/To/jar/3.jar"

# ã€å£çº¸ã€‘
WALLPAPER_URL = "https://api.kdcc.cn"

# ã€äº²ç”Ÿå®¿ä¸»åˆ—è¡¨ã€‘
# ä¼˜å…ˆæŠ“å–ï¼Œä¸”ä¿ç•™å…¶ Spider æ¥å£
COMPATIBLE_HOSTS = [
    "http://www.é¥­å¤ªç¡¬.com/tv",
    "http://è‚¥çŒ«.com",
    "http://fty.xxooo.cf/tv",
    "http://rihou.cc:88/è·åŸèŒ¶ç§€"
]

# ã€æœåˆ®åˆ—è¡¨ã€‘(æ–°å¢é“é•¿å®˜æ–¹æº)
EXTERNAL_URLS = COMPATIBLE_HOSTS + [
    # --- é“é•¿ dr_py å®˜æ–¹æº (ä½ åˆšæ‰å‘çš„ä»£ç çš„æºå¤´) ---
    "https://raw.githubusercontent.com/hjdhnx/dr_py/main/tvbox.json",
    
    # --- å…¶ä»–ä¼˜è´¨å¤§å‚ ---
    "https://api.hgyx.vip/hgyx.json",                  # éŸ©å›½ä½¬
    "https://tv.èœå¦®ä¸.top",                           # èœå¦®ä¸
    "https://raw.githubusercontent.com/yoursmile66/TVBox/main/XC.json", # å—é£
    "https://raw.githubusercontent.com/guot55/YGBH/main/vip2.json",     # å®ç›’
    "https://cnb.cool/fish2018/duanju/-/git/raw/main/tvbox.json",       # çŸ­å‰§
    "https://raw.githubusercontent.com/chitue/dongliTV/main/api.json",  # åŠ¨åŠ›
    "https://cdn.gitmirror.com/bb/xduo/libs/master/index.json",          # é“é•¿é•œåƒ
    "https://ghproxy.net/https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json", # é«˜å¤©æµäº‘
    "https://www.252035.xyz/z/FongMi.json",            # FongMi
    "http://52bsj.vip:81/api/v3/file/get/29899/bsj2023.json?sign=3c594b2b985b365bad", # è¿è¾“è½¦
    
    # --- æ•£æˆ·æ±  ---
    "http://ok321.top/tv",
    "http://tvbox.ç‹äºŒå°æ”¾ç‰›å¨ƒ.top",
    "https://android.lushunming.qzz.io/json/index.json",
    "http://home.jundie.top:81/top98.json",
    "https://s2.pub/x",
    "http://tv.nxog.top/m/111.php?ou=å…¬ä¼—å·æ¬§æ­Œapp&mz=all&jar=all&b=æ¬§æ­Œ",
    "https://100km.top/0",
    "http://meowtv.cn/tv",
    "http://cdn.qiaoji8.com/tvbox.json" 
]

# ã€è¿‡æ»¤é…ç½®ã€‘
ALLOWED_TYPES = [0, 1, 3, 4] 

# ã€å¹¿å‘Š/åƒåœ¾ é»‘åå•ã€‘
BLACKLIST = [
    "å¤±æ•ˆ", "æµ‹è¯•", "å¹¿å‘Š", "æ”¶è´¹", "ç¾¤", "åŠ V", "æŒ‚å£", "Qç¾¤", "ä¼¦ç†", "ç¦åˆ©", "æˆäºº", "æƒ…è‰²", 
    "å¼•æµ", "æ›´æ–°", "æ‰«ç ", "å¾®ä¿¡", "ä¼é¹…", "APP", "ä¸‹è½½", "æ¨å¹¿", "éªŒè¯", "æ¿€æ´»", "æˆæƒ", 
    "é›·é²¸", "ç©å¶å“¥å“¥", "åŠ©æ‰‹", "ä¸“çº¿", "å½©è›‹", "ç›´æ’­", "77.110", "mingming", "æ‘¸é±¼"
]

# ã€ç½‘ç›˜/Alist ç‰¹å¾è¯ã€‘(ç²¾å‡†å‰”é™¤ä½ ä¸éœ€è¦çš„ç½‘ç›˜)
# åªè¦ API æˆ– åå­— é‡ŒåŒ…å«è¿™äº›ï¼Œç›´æ¥æ€æ‰
DISK_KEYWORDS = [
    "é˜¿é‡Œäº‘", "å¤¸å…‹", "UCç½‘ç›˜", "115", "ç½‘ç›˜", "äº‘ç›˜", "æ¨é€", "å­˜å‚¨", 
    "Drive", "Ali", "Quark", "Alist", "1359527.xyz" # å±è”½é“é•¿çš„ç§æœ‰æœåŠ¡å™¨(ä¸ç¨³å®š)
]

TIMEOUT = 15       
MAX_WORKERS = 50   

# ================= 2. å·¥å…·å‡½æ•° =================

def decode_content(content):
    if not content: return None
    try: return json.loads(content)
    except: pass
    try:
        clean = content.replace('**', '').replace('o', '').strip()
        return json.loads(base64.b64decode(clean).decode('utf-8'))
    except:
        try:
            match = re.search(r'\{.*\}', content, re.DOTALL)
            if match: return json.loads(match.group())
        except: pass
    return None

def get_json(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        # verify=False å¿½ç•¥è¯ä¹¦é”™è¯¯ï¼Œå¢åŠ æˆåŠŸç‡
        res = requests.get(url, headers=headers, timeout=TIMEOUT, verify=False)
        res.encoding = 'utf-8'
        if res.status_code == 200:
            return decode_content(res.text)
    except: pass
    return None

def clean_name(name):
    name = str(name)
    name = re.sub(r'ã€.*?ã€‘|\[.*?\]|\(.*?\)|ï¼ˆ.*?ï¼‰', '', name)
    name = name.replace("èšåˆ", "").replace("è“å…‰", "").replace("ä¸“çº¿", "").strip()
    return name

# ================= 3. æ ¸å¿ƒå¤„ç†é€»è¾‘ =================

def fetch_and_process(url):
    data = get_json(url)
    if not data: return []
    
    extracted_sites = []
    
    # åˆ¤å®šæ˜¯å¦ä¸ºäº²ç”Ÿå®¿ä¸»
    is_compatible_host = False
    for host in COMPATIBLE_HOSTS:
        if host in url:
            is_compatible_host = True
            break
    
    def process_site(site):
        # 1. å¼ºåˆ¶å‰¥ç¦» Jar (é˜²æ­¢é—ªé€€ï¼Œç»Ÿä¸€ç”¨é¥­å¤ªç¡¬æ ¸å¿ƒ)
        if 'jar' in site:
            del site['jar']
            
        name = site.get('name', '')
        api = str(site.get('api', ''))
        stype = site.get('type', 0)
        
        # 2. ã€æ ¸å¿ƒã€‘ç½‘ç›˜ & Alist è¿‡æ»¤
        # é“é•¿çš„é…ç½®é‡Œæœ‰å¾ˆå¤š "Alist(xx)" å’Œ "http://.../alist.min.js"
        # è¿™é‡Œç»Ÿä¸€æŸ¥æ€
        is_disk = False
        # æŸ¥åå­—
        if any(k in name for k in DISK_KEYWORDS): is_disk = True
        # æŸ¥APIé“¾æ¥ (ä¸åŒºåˆ†å¤§å°å†™)
        if not is_disk:
            api_lower = api.lower()
            if any(k.lower() in api_lower for k in DISK_KEYWORDS): is_disk = True
            
        if is_disk:
            # print(f"       [x] å‰”é™¤ç½‘ç›˜/Alist: {name}")
            return None

        # 3. å¹¿å‘Šè¿‡æ»¤
        if any(bw in name for bw in BLACKLIST): return None
        if any(char in name for char in ['ğŸ’°', 'ğŸ‘—', 'ğŸ‘ ', 'âœ¨', 'âš¡', 'ğŸ”¥', 'å…è´¹', 'é€', 'åŠ V']): return None
        
        # 4. é˜²å´©è¿‡æ»¤
        # å¦‚æœæ˜¯å¤–éƒ¨æºçš„ Spider (Type 3)ï¼Œä¸”ä¸æ˜¯æ¥è‡ªäº²ç”Ÿå®¿ä¸»
        # ä¸ºäº†é˜²æ­¢ä¸å…¼å®¹ï¼Œå»ºè®®è¿‡æ»¤ã€‚ä½†å¦‚æœä½ æƒ³èµŒå®ƒèƒ½ç”¨ï¼Œå¯ä»¥æ³¨é‡Šæ‰ä¸‹é¢ä¸¤è¡Œã€‚
        # (é“é•¿çš„ drpy å¾ˆå¤šéœ€è¦ä»–çš„ç§æœ‰æœåŠ¡å™¨ï¼Œè¿™é‡Œä¸ºäº†ç¨³å®šï¼Œå¦‚æœä¸å…¼å®¹å°±ä¸¢å¼ƒ)
        if stype == 3 and not is_compatible_host:
             # ä½†æ˜¯ï¼Œä¸ºäº†ä¸é”™è¿‡å¥½èµ„æºï¼Œæˆ‘ä»¬æ”¾å®½ä¸€ç‚¹ï¼š
             # å¦‚æœæ˜¯ drpy ç±»å‹çš„ï¼Œä¸”ç”¨äº†å¤–éƒ¨ JSï¼Œå¯èƒ½ä¸å…¼å®¹ã€‚
             # è¿™é‡Œæˆ‘ä»¬é‡‡å–â€œè¯•æ¢æ€§ä¿ç•™â€ï¼Œä¸å¼ºåˆ¶æ€æ‰ï¼Œçœ‹çœ‹é¥­å¤ªç¡¬ Jar èƒ½ä¸èƒ½æ‰›ä½ã€‚
             pass 
        
        # 5. æ ‡è®°ä¸ç¾åŒ–
        site['name'] = clean_name(name)
        site['searchable'] = 1 
        site['quickSearch'] = 1
        
        if stype == 3:
            site['name'] = f"ğŸ›¡ï¸ {site['name']}" # Spider
        else:
            site['name'] = f"ğŸš€ {site['name']}" # CMS/App
            
        return site

    # æå–å¤šä»“
    if 'urls' in data and isinstance(data['urls'], list):
        for item in data['urls']:
            if 'url' in item:
                sub_data = get_json(item['url'])
                if sub_data and 'sites' in sub_data:
                    for s in sub_data['sites']:
                        processed = process_site(s)
                        if processed: extracted_sites.append(processed)
    
    # æå–å•ä»“
    if 'sites' in data:
        for s in data['sites']:
            processed = process_site(s)
            if processed: extracted_sites.append(processed)
            
    return extracted_sites

def main():
    try:
        requests.packages.urllib3.disable_warnings()
        print(">>> å¯åŠ¨ TVBox v39.0 (å»Alist/å¸çº³é“é•¿/é¥­å¤ªç¡¬æ ¸å¿ƒ)")
        
        all_sites = []
        unique_urls = list(set(EXTERNAL_URLS))
        
        # 1. å¹¶å‘æŠ“å–
        print(f">>> [1/2] æ­£åœ¨èšåˆ {len(unique_urls)} ä¸ªè®¢é˜…æº...")
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_url = {executor.submit(fetch_and_process, url): url for url in unique_urls}
            for future in concurrent.futures.as_completed(future_to_url):
                try:
                    sites = future.result()
                    if sites: all_sites.extend(sites)
                except: pass
        
        # 2. å»é‡ä¸ç”Ÿæˆ
        print(f">>> [2/2] å»é‡ä¸æ‰“åŒ…...")
        unique_sites = []
        seen_api = set()
        
        # ä¼˜å…ˆä¿ç•™æ’åœ¨å‰é¢çš„æº
        for s in all_sites:
            api = s.get('api', '')
            if api and api not in seen_api:
                unique_sites.append(s)
                seen_api.add(api)
                
        # 3. æˆªæ–­ (ä¿ç•™300ä¸ª)
        max_sites = 300
        if len(unique_sites) > max_sites:
            unique_sites = unique_sites[:max_sites]
        
        # 4. ç”Ÿæˆé…ç½®
        config = {
            "spider": GLOBAL_SAFE_JAR, # é¥­å¤ªç¡¬å®˜æ–¹ HTTP Jar
            "wallpaper": WALLPAPER_URL,
            "sites": unique_sites,
            "lives": [],
            "parses": [],
            "flags": []
        }
        
        with open("my_tvbox.json", 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
            
        print(f"\nâœ… å®Œæˆï¼")
        print(f"ğŸ“Š èšåˆæ¥å£: {len(unique_sites)} ä¸ª")
        print(f"ğŸ§¹ å·²å‰”é™¤: Alist/ç½‘ç›˜/é“é•¿ç§æœ‰æœåŠ¡å™¨æ¥å£")
        print(f"ğŸ›¡ï¸ æ ¸å¿ƒ Jar: {GLOBAL_SAFE_JAR}")
        
    except Exception as e:
        print(f"\n[!!!] é”™è¯¯: {e}")
        if not os.path.exists("my_tvbox.json"):
            with open("my_tvbox.json", 'w', encoding='utf-8') as f:
                json.dump({"spider":GLOBAL_SAFE_JAR, "sites":[]}, f)
        sys.exit(0)

if __name__ == "__main__":
    main()
