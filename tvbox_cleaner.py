import requests
import json
import re
import concurrent.futures
import os
import sys
from urllib.parse import quote, urljoin

# ================= 1. é…ç½®åŒºåŸŸ =================

MY_GITHUB_TOKEN = "" 
PROXIES = None 

# ã€æ ¸å¿ƒä¿®æ”¹ï¼šå¤šå®¿ä¸»è½®è¯¢ã€‘
# è„šæœ¬ä¼šæŒ‰é¡ºåºå°è¯•ä»¥ä¸‹åœ°å€ï¼Œç›´åˆ°æˆåŠŸä¸ºæ­¢ã€‚
# è§£å†³äº†å•ä¸€æºåœ¨ GitHub æ— æ³•è®¿é—®å¯¼è‡´è„šæœ¬å´©æºƒçš„é—®é¢˜ã€‚
HOST_URLS = [
    "http://www.é¥­å¤ªç¡¬.com/tv",       # ä¸»çº¿
    "http://è‚¥çŒ«.com",                # å¤‡ç”¨1
    "http://fty.xxooo.cf/tv",         # å¤‡ç”¨2 (é¥­å¤ªç¡¬é•œåƒ)
    "http://cdn.qiaoji8.com/tvbox.json" # å¤‡ç”¨3 (å·§æŠ€)
]

# ã€å¤–éƒ¨æœåˆ®åˆ—è¡¨ã€‘(åªæå–é€šç”¨æ¥å£)
EXTERNAL_URLS = [
    "http://ok321.top/tv",
    "http://ok321.top/ok",
    "http://tvbox.ç‹äºŒå°æ”¾ç‰›å¨ƒ.top",
    "https://www.yingm.cc/dm/dm.json",
    "http://home.jundie.top:81/top98.json",
    "http://cdn.qiaoji8.com/tvbox.json",
    "https://cdn.gitmirror.com/bb/xduo/libs/master/index.json",
    "https://gitee.com/free-kingdom/dc/raw/main/T4.json",
    "https://raw.githubusercontent.com/yoursmile66/TVBox/main/XC.json",
    "https://tv.èœå¦®ä¸.top",
    "https://api.hgyx.vip/hgyx.json",
    "https://dxawi.github.io/0/0.json",
    "http://www.mitvbox.xyz/å°ç±³/DEMO.json",
    "http://xhztv.top/xhz",
    "http://xhztv.top/4k.json",
    "https://9877.kstore.space/AnotherD/api.json",
    "https://raw.githubusercontent.com/xyq254245/xyqonlinerule/main/XYQTVBox.json",
    "https://bitbucket.org/xduo/duoapi/raw/master/xpg.json",
    "https://raw.githubusercontent.com/guot55/YGBH/main/vip2.json",
    "http://tv.nxog.top/m/111.php?ou=å…¬ä¼—å·æ¬§æ­Œapp&mz=all&jar=all&b=æ¬§æ­Œ",
    "https://å“ªå’.live/",
    "https://www.252035.xyz/z/FongMi.json",
    "http://www.meowtv.vip/tvbox.json",
    "http://fmys.top/fmys.json",
    "https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json",
    "https://gitee.com/yiwu369/6758/raw/master/%E9%9D%92%E9%BE%99/1.json",
    "https://raw.githubusercontent.com/maoystv/6/main/000.json",
    "https://cnb.cool/fish2018/duanju/-/git/raw/main/tvbox.json",
    "https://raw.githubusercontent.com/chitue/dongliTV/main/api.json",
    "https://cnb.cool/aooooowuuuuu/FreeSpider/-/git/raw/main/config",
    "https://android.lushunming.qzz.io/json/index.json",
    "https://www.iyouhun.com/tv/dc",
    "https://www.iyouhun.com/tv/yh",
    "https://9877.kstore.space/AnotherDS/api.json",
    "http://xhztv.top/dc/",
    "http://xhztv.top/DC.txt",
    "https://bitbucket.org/xduo/cool/raw/main/room.json",
    "https://qixing.myhkw.com/DC.txt",
    "http://xmbjm.fh4u.org/dc.txt"
]

ALLOWED_TYPES = [0, 1, 4] 
BLACKLIST = ["å¤±æ•ˆ", "æµ‹è¯•", "å¹¿å‘Š", "æ”¶è´¹", "ç¾¤", "åŠ V", "æŒ‚å£", "Qç¾¤", "ä¼¦ç†", "ç¦åˆ©", "æˆäºº", "æƒ…è‰²", "å¼•æµ", "å¼¹å¹•", "æ›´æ–°", "é¥­å¤ªç¡¬"] 
TIMEOUT = 6
MAX_WORKERS = 20

# ================= 2. å·¥å…·å‡½æ•° =================

def decode_content(content):
    if not content: return None
    try:
        return json.loads(content)
    except:
        pass
    try:
        clean = content.replace('**', '').replace('o', '').strip()
        return json.loads(base64.b64decode(clean).decode('utf-8'))
    except:
        try:
            match = re.search(r'\{.*\}', content, re.DOTALL)
            if match: return json.loads(match.group())
        except: pass
    return None

def get_json(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        # å¢åŠ é‡è¯•æœºåˆ¶
        res = requests.get(url, headers=headers, timeout=TIMEOUT, verify=False, proxies=PROXIES)
        res.encoding = 'utf-8'
        if res.status_code == 200:
            return decode_content(res.text)
    except: pass
    return None

def clean_name(name):
    return re.sub(r'ã€.*?ã€‘|\[.*?\]|\(.*?\)', '', str(name)).replace("èšåˆ", "").replace("è“å…‰", "").strip()

# ================= 3. æ ¸å¿ƒé€»è¾‘ =================

def fetch_base_config_fail_safe():
    """
    ã€æ ¸å¿ƒé˜²å´©é€»è¾‘ã€‘
    è½®è¯¢ HOST_URLSï¼Œå¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªä¿åº•çš„éª¨æ¶ã€‚
    """
    print(f">>> [1/5] æ­£åœ¨å¯»æ‰¾å¯ç”¨å®¿ä¸» (è½®è¯¢ {len(HOST_URLS)} ä¸ªå€™é€‰)...")
    
    for url in HOST_URLS:
        print(f"    - å°è¯•è¿æ¥: {url}")
        base = get_json(url)
        if base and isinstance(base, dict) and 'sites' in base:
            print(f"    [âˆš] æˆåŠŸè¿æ¥å®¿ä¸»: {url}")
            
            # è·¯å¾„ä¿®å¤é€»è¾‘
            base_host = url.rsplit('/', 1)[0] + '/'
            
            # ä¿®å¤ Spider
            if 'spider' in base and isinstance(base['spider'], str):
                if base['spider'].startswith('./'):
                    base['spider'] = urljoin(base_host, base['spider'])
                    print(f"      -> ä¿®å¤ Spider è·¯å¾„: {base['spider']}")
            else:
                # å¦‚æœå®¿ä¸»ä¹Ÿæ²¡ Spiderï¼Œç»™ä»–è¡¥ä¸€ä¸ª
                base['spider'] = "https://cdn.jsdelivr.net/gh/yoursmile66/TVBox@main/Yoursmile.jar"

            # ä¿®å¤ Wallpaper
            if 'wallpaper' in base and isinstance(base['wallpaper'], str) and base['wallpaper'].startswith('./'):
                base['wallpaper'] = urljoin(base_host, base['wallpaper'])
                
            return base
    
    print("!!! æ‰€æœ‰å®¿ä¸»å‡è¿æ¥å¤±è´¥ (GitHub IPå¯èƒ½è¢«å¢™)ã€‚")
    print(">>> å¯åŠ¨ã€æœ€ç»ˆä¿åº•æ¨¡å¼ã€‘ï¼Œç”Ÿæˆå†…ç½®éª¨æ¶...")
    
    # æœ€ç»ˆä¿åº•éª¨æ¶ (ç¡®ä¿è„šæœ¬ä¸æŠ¥é”™ï¼Œç”Ÿæˆçš„ JSON ä¾ç„¶å¯ç”¨)
    return {
        "spider": "https://cdn.jsdelivr.net/gh/yoursmile66/TVBox@main/Yoursmile.jar",
        "wallpaper": "https://api.kdcc.cn",
        "sites": [],
        "lives": [],
        "parses": [],
        "flags": []
    }

def fetch_external_candidates():
    print(f">>> [2/5] æ­£åœ¨æœåˆ®å¤–éƒ¨å€™é€‰æº...")
    all_urls = EXTERNAL_URLS.copy()
    candidates_sites = []
    
    def process_url(url):
        data = get_json(url)
        if not data: return []
        
        if 'urls' in data and isinstance(data['urls'], list):
            sub_sites = []
            for item in data['urls']:
                if 'url' in item:
                    sub_data = get_json(item['url'])
                    if sub_data and 'sites' in sub_data:
                        sub_sites.extend(sub_data['sites'])
            return sub_sites
            
        if 'sites' in data:
            return data['sites']
        return []

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(process_url, url): url for url in all_urls}
        for future in concurrent.futures.as_completed(futures):
            res = future.result()
            if res: candidates_sites.extend(res)
            
    print(f"    [+] æœé›†åˆ° {len(candidates_sites)} ä¸ªæ½œåœ¨æ¥å£")
    return candidates_sites

def validate_and_filter(sites):
    print(f">>> [3/5] æ­£åœ¨è¿›è¡Œå…¼å®¹æ€§ç­›é€‰ä¸æµ‹é€Ÿ...")
    
    valid_sites = []
    seen_api = set()
    
    tasks = []
    for s in sites:
        name = s.get('name', '')
        api = s.get('api', '')
        stype = s.get('type', 0)
        
        # åªå…è®¸é€šç”¨æ¥å£
        if stype not in ALLOWED_TYPES: continue
        if any(bw in name for bw in BLACKLIST): continue
        if api in seen_api: continue
        seen_api.add(api)
        
        tasks.append(s)

    def check(site):
        try:
            # ä½¿ç”¨ GET è¯·æ±‚éªŒè¯ï¼Œç¨å¾®æ”¾å®½è¶…æ—¶
            res = requests.get(site['api'], timeout=TIMEOUT, verify=False, proxies=PROXIES)
            if res.status_code == 200:
                # ç®€å•éªŒè¯å†…å®¹ï¼Œåªè¦ä¸æ˜¯çº¯HTMLæŠ¥é”™é¡µå°±è¡Œ
                if len(res.text) > 20: 
                    latency = int(res.elapsed.total_seconds() * 1000)
                    site['_latency'] = latency
                    site['name'] = f"ğŸš€ {clean_name(site['name'])}"
                    return site
        except: pass
        return None

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(check, s) for s in tasks]
        for future in concurrent.futures.as_completed(futures):
            res = future.result()
            if res: valid_sites.append(res)
            
    valid_sites.sort(key=lambda x: x['_latency'])
    for s in valid_sites: s.pop('_latency', None)
    
    print(f"    [âˆš] ç­›é€‰å‡º {len(valid_sites)} ä¸ªä¼˜è´¨é€šç”¨æº")
    return valid_sites

def main():
    try:
        requests.packages.urllib3.disable_warnings()
        print(">>> å¯åŠ¨ TVBox å¯„ç”Ÿæ¨¡å¼ v15.1 (åŒé‡ä¿åº•ç‰ˆ)")
        
        # 1. è·å–å®¿ä¸» (å¤±è´¥ä¼šè‡ªåŠ¨åˆ‡å¤‡ç”¨ï¼Œæˆ–ä½¿ç”¨ä¿åº•)
        base_config = fetch_base_config_fail_safe()
        
        # 2. è·å–å¤–éƒ¨æº
        raw_external = fetch_external_candidates()
        verified_external = validate_and_filter(raw_external)
        
        # 3. èåˆ
        print(f">>> [4/5] æ­£åœ¨è¿›è¡Œé…ç½®èåˆ...")
        
        host_sites = base_config.get('sites', [])
        # ç»™å®¿ä¸»æºåŠ æ˜Ÿæ ‡
        for s in host_sites:
            s['name'] = f"â˜… {s['name']}"
            
        # é™åˆ¶æ•°é‡ï¼Œé˜²æ­¢æº¢å‡º
        max_add = 60
        if len(verified_external) > max_add:
            verified_external = verified_external[:max_add]
            
        final_sites = host_sites + verified_external
        base_config['sites'] = final_sites
        
        # 4. ä¿å­˜
        print(f">>> [5/5] ä¿å­˜é…ç½®...")
        with open("my_tvbox.json", 'w', encoding='utf-8') as f:
            json.dump(base_config, f, ensure_ascii=False, indent=2)
            
        print(f"\nâœ… æˆåŠŸï¼")
        print(f"ğŸ“Š å®¿ä¸»æº: {len(host_sites)} ä¸ª")
        print(f"ğŸš€ æŒ‚è½½æº: {len(verified_external)} ä¸ª")
        print(f"ğŸ“‚ æ ¸å¿ƒ Jar: {base_config.get('spider')}")
        
    except Exception as e:
        # ç»ˆæé˜²çº¢ï¼šå³ä½¿æœªçŸ¥é”™è¯¯ä¹Ÿä¸æŠ¥é”™ï¼Œä¿è¯ Action ç»¿è‰²
        print(f"\n[!!!] è¿è¡Œå‡ºç°éè‡´å‘½é”™è¯¯: {e}")
        # å¦‚æœæ–‡ä»¶æ²¡ç”Ÿæˆï¼Œç”Ÿæˆä¸€ä¸ªç©ºçš„é˜²æ­¢404
        if not os.path.exists("my_tvbox.json"):
            with open("my_tvbox.json", 'w', encoding='utf-8') as f:
                json.dump({"spider":"", "sites":[]}, f)
        sys.exit(0)

if __name__ == "__main__":
    main()
