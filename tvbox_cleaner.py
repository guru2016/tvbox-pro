import requests
import json
import re
import concurrent.futures
import os
import sys
from urllib.parse import quote, urljoin

# ================= 1. é…ç½®åŒºåŸŸ =================

MY_GITHUB_TOKEN = "" 
PROXIES = None 

# ã€æ ¸å¿ƒå®‰å…¨é”ã€‘
# å¼ºåˆ¶ä½¿ç”¨ä½ è‡ªå·±çš„çº¯å‡€ Jarï¼Œè¿™æ˜¯æœç»å¹¿å‘Šçš„æ ¹æœ¬
GITHUB_USER = "guru2016"
REPO_NAME = "tvbox-pro"
BRANCH_NAME = "main"
SAFE_JAR_URL = f"https://cdn.jsdelivr.net/gh/{GITHUB_USER}/{REPO_NAME}@{BRANCH_NAME}/spider.jar"

# ã€å®¿ä¸»é…ç½®ã€‘(åªç”¨é¥­å¤ªç¡¬åšéª¨æ¶ï¼Œæœ€ç¨³)
HOST_URLS = [
    "http://www.é¥­å¤ªç¡¬.com/tv",       
    "http://è‚¥çŒ«.com",
    "http://fty.xxooo.cf/tv"
]

# ã€æœåˆ®åˆ—è¡¨ã€‘(è¿™é‡Œæ”¾ä½ æƒ³â€œå¸â€çš„æºï¼ŒåŒ…æ‹¬å·§æŠ€)
EXTERNAL_URLS = [
    # --- é‡ç‚¹å¸è¡€å¯¹è±¡ ---
    "http://cdn.qiaoji8.com/tvbox.json",  # å·§æŠ€ (åªå¸é€šç”¨æ¥å£)
    "http://tv.nxog.top/m/111.php?ou=å…¬ä¼—å·æ¬§æ­Œapp&mz=all&jar=all&b=æ¬§æ­Œ", # æ¬§æ­Œ
    "https://api.hgyx.vip/hgyx.json",     # éŸ©å›½ä½¬
    "https://tv.èœå¦®ä¸.top",              # èœå¦®ä¸
    
    # --- æ–°å¢ä¼˜è´¨å¤§å‚ ---
    "https://cdn.gitmirror.com/bb/xduo/libs/master/index.json", # é“é•¿
    "https://raw.githubusercontent.com/yoursmile66/TVBox/main/XC.json", # å—é£
    "https://raw.githubusercontent.com/guot55/YGBH/main/vip2.json", # å®ç›’
    "https://cnb.cool/fish2018/duanju/-/git/raw/main/tvbox.json",   # çŸ­å‰§
    "http://tvbox.ç‹äºŒå°æ”¾ç‰›å¨ƒ.top",
    "http://ok321.top/tv",
    "http://52bsj.vip:81/api/v3/file/get/29899/bsj2023.json?sign=3c594b2b985b365bad", # è¿è¾“è½¦
    
    # --- å¤‡ç”¨æ±  ---
    "https://www.252035.xyz/z/FongMi.json",
    "http://www.meowtv.vip/tvbox.json",
    "https://raw.githubusercontent.com/chitue/dongliTV/main/api.json",
    "https://android.lushunming.qzz.io/json/index.json"
]

# ã€è¿‡æ»¤é…ç½®ã€‘(ä¸¥é˜²æ­»å®ˆ)
# åªå…è®¸ 0(xml), 1(json), 4(app/parse)
# ç»å¯¹ä¸è¦ 3(spider)ï¼Œå› ä¸ºæˆ‘ä»¬ä¸ç”¨ä»–ä»¬çš„Jar
ALLOWED_TYPES = [0, 1, 4] 

# ã€æ€æ¯’é»‘åå•ã€‘
BLACKLIST = [
    "å¤±æ•ˆ", "æµ‹è¯•", "å¹¿å‘Š", "æ”¶è´¹", "ç¾¤", "åŠ V", "æŒ‚å£", "Qç¾¤", "ä¼¦ç†", "ç¦åˆ©", "æˆäºº", "æƒ…è‰²", 
    "å¼•æµ", "å¼¹å¹•", "æ›´æ–°", "é¥­å¤ªç¡¬", "å…¬ä¼—å·", "æ‰«ç ", "å¾®ä¿¡", "ä¼é¹…", "APP", "ä¸‹è½½", 
    "è“å…‰", "äº’åŠ©", "æ¨å¹¿", "éªŒè¯", "æ¿€æ´»", "æˆæƒ", "é›·é²¸", "å°è„‘æ–§", "ç©å¶", "åŠ©æ‰‹", 
    "ä¸“çº¿", "å‰§ç™½ç™½", "å‰§è’", "è…¾äº‘", "å½©è›‹", "ç¥é©¬", "æ‚¦äº«"
]

TIMEOUT = 5
MAX_WORKERS = 15

# ================= 2. å·¥å…·å‡½æ•° =================

def decode_content(content):
    if not content: return None
    try:
        return json.loads(content)
    except:
        pass
    try:
        clean = content.replace('**', '').replace('o', '').strip()
        return json.loads(base64.b64decode(clean).decode('utf-8'))
    except:
        try:
            match = re.search(r'\{.*\}', content, re.DOTALL)
            if match: return json.loads(match.group())
        except: pass
    return None

def get_json(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, timeout=TIMEOUT, verify=False, proxies=PROXIES)
        res.encoding = 'utf-8'
        if res.status_code == 200:
            return decode_content(res.text)
    except: pass
    return None

def clean_name(name):
    name = str(name)
    name = re.sub(r'ã€.*?ã€‘|\[.*?\]|\(.*?\)|ï¼ˆ.*?ï¼‰', '', name)
    name = name.replace("èšåˆ", "").replace("è“å…‰", "").replace("ä¸“çº¿", "").strip()
    return name

# ================= 3. æ ¸å¿ƒé€»è¾‘ =================

def fetch_base_config_fail_safe():
    print(f">>> [1/5] æ­£åœ¨è¿æ¥å®¿ä¸» (é¥­å¤ªç¡¬/è‚¥çŒ«)...")
    
    # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦é…ç½®äº† GITHUB_USER
    if "guru2016" not in SAFE_JAR_URL:
        print("!!! è­¦å‘Šï¼šè¯·åœ¨è„šæœ¬å¤´éƒ¨å¡«å†™æ­£ç¡®çš„ GITHUB_USERï¼Œå¦åˆ™ Jar åŒ…æ— æ³•åŠ è½½ï¼")

    for url in HOST_URLS:
        print(f"    - å°è¯•: {url}")
        base = get_json(url)
        if base and isinstance(base, dict) and 'sites' in base:
            print(f"    [âˆš] æˆåŠŸé”å®šå®¿ä¸»: {url}")
            base_host = url.rsplit('/', 1)[0] + '/'
            
            # ã€æ ¸å¿ƒç­–ç•¥ã€‘å¼ºåˆ¶è¦†ç›– Jarï¼Œä¸è®ºå®¿ä¸»æ˜¯è°
            print(f"    [ğŸ›¡ï¸] æ³¨å…¥çº¯å‡€é˜²æ¯’ Jar: {SAFE_JAR_URL}")
            base['spider'] = SAFE_JAR_URL

            if 'wallpaper' in base and isinstance(base['wallpaper'], str) and base['wallpaper'].startswith('./'):
                base['wallpaper'] = urljoin(base_host, base['wallpaper'])
                
            return base
    
    print("!!! å®¿ä¸»è¿æ¥å…¨éƒ¨å¤±è´¥ï¼Œå¯ç”¨æœ¬åœ°ä¿åº•éª¨æ¶ã€‚")
    return {
        "spider": SAFE_JAR_URL,
        "wallpaper": "https://api.kdcc.cn",
        "sites": [], "lives": [], "parses": [], "flags": []
    }

def fetch_external_candidates():
    print(f">>> [2/5] æ­£åœ¨å…¨ç½‘æ”¶å‰²ä¼˜è´¨æ¥å£...")
    all_urls = EXTERNAL_URLS.copy()
    candidates_sites = []
    
    def process_url(url):
        data = get_json(url)
        if not data: return []
        
        extracted = []
        # æå–å¤šä»“
        if 'urls' in data and isinstance(data['urls'], list):
            for item in data['urls']:
                if 'url' in item:
                    sub_data = get_json(item['url'])
                    if sub_data and 'sites' in sub_data:
                        extracted.extend(sub_data['sites'])
        # æå–å•ä»“
        if 'sites' in data:
            extracted.extend(data['sites'])
            
        return extracted

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(process_url, url): url for url in all_urls}
        for future in concurrent.futures.as_completed(futures):
            res = future.result()
            if res: candidates_sites.extend(res)
            
    print(f"    [+] å…±æ”¶é›†åˆ° {len(candidates_sites)} ä¸ªæ½œåœ¨æ¥å£")
    return candidates_sites

def validate_and_filter(sites):
    print(f">>> [3/5] æ­£åœ¨è¿›è¡Œ å®‰å…¨æ¸…æ´— & æ·±åº¦æµ‹é€Ÿ...")
    
    valid_sites = []
    seen_api = set()
    tasks = []
    
    for s in sites:
        name = s.get('name', '')
        api = s.get('api', '')
        stype = s.get('type', 0)
        
        # 1. ã€æ ¸å¿ƒè¿‡æ»¤ã€‘åšå†³ä¸è¦ Type 3 (Spider)
        # è¿™æ˜¯é˜²æ­¢å·§æŠ€ç­‰æºå¼¹çª—ã€é—ªé€€çš„æ ¹æœ¬ï¼
        if stype not in ALLOWED_TYPES: continue
        
        # 2. å»é‡
        if api in seen_api: continue
        
        # 3. å…³é”®è¯é»‘åå•
        if any(bw in name for bw in BLACKLIST): continue
        
        # 4. æ’é™¤ emoji å¹¿å‘Š (åå­—å¤ªèŠ±å“¨çš„ä¸€èˆ¬éƒ½æ˜¯å¹¿)
        if any(char in name for char in ['ğŸ’°', 'ğŸ‘—', 'ğŸ‘ ', 'âœ¨', 'âš¡', 'ğŸ”¥', 'å…è´¹', 'é€', 'åŠ V']):
            continue

        seen_api.add(api)
        tasks.append(s)

    # æ·±åº¦æµ‹é€Ÿ
    def check(site):
        try:
            # å¿…é¡»æ˜¯çœŸå®çš„ JSON æ¥å£ï¼Œä¸èƒ½æ˜¯ç½‘é¡µ
            res = requests.get(site['api'], timeout=TIMEOUT, verify=False, proxies=PROXIES)
            if res.status_code == 200:
                # ç®€å•éªŒè¯å†…å®¹ï¼Œé˜²æ­¢ç©ºå£³
                content = res.text.strip()
                if content.startswith('{') or content.startswith('['):
                    # åªè¦èƒ½é€šï¼Œä¸”æ˜¯JSONï¼Œå°±è®¤ä¸ºæ˜¯å¥½æº
                    latency = int(res.elapsed.total_seconds() * 1000)
                    site['_latency'] = latency
                    site['name'] = f"ğŸš€ {clean_name(site['name'])}"
                    return site
        except: pass
        return None

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(check, s) for s in tasks]
        for future in concurrent.futures.as_completed(futures):
            res = future.result()
            if res: valid_sites.append(res)
            
    # æŒ‰é€Ÿåº¦æ’åºï¼Œè¶Šå¿«è¶Šå‰
    valid_sites.sort(key=lambda x: x['_latency'])
    for s in valid_sites: s.pop('_latency', None)
    
    print(f"    [âˆš] æ¸…æ´—å®Œæ¯•ï¼Œå‰©ä½™ {len(valid_sites)} ä¸ªçº¯å‡€é€šç”¨æº")
    return valid_sites

def main():
    try:
        requests.packages.urllib3.disable_warnings()
        print(">>> å¯åŠ¨ TVBox å¸æ˜Ÿå¤§æ³• v18.0")
        
        # 1. æå®šå®¿ä¸» (å¸¦é˜²æ¯’Jar)
        base_config = fetch_base_config_fail_safe()
        
        # 2. å…¨ç½‘æ”¶å‰²
        raw_external = fetch_external_candidates()
        verified_external = validate_and_filter(raw_external)
        
        print(f">>> [4/5] èåˆé…ç½®...")
        host_sites = base_config.get('sites', [])
        
        # å¤„ç†å®¿ä¸»æºåå­—
        # æ³¨æ„ï¼šå®¿ä¸»(é¥­å¤ªç¡¬)é‡Œçš„ Type 3 æºå¯ä»¥ä¿ç•™ï¼Œå› ä¸ºæˆ‘ä»¬çš„çº¯å‡€ Jar å¤§æ¦‚ç‡å…¼å®¹ä»–çš„æº
        # ä½†å¦‚æœå‘ç°å®¿ä¸»é‡Œçš„æºä¹Ÿæœ‰å¼¹çª—ï¼Œå¯ä»¥åœ¨è¿™é‡ŒåŠ é€»è¾‘è¿‡æ»¤
        safe_host_sites = []
        for s in host_sites:
            s['name'] = f"â˜… {clean_name(s['name'])}"
            safe_host_sites.append(s)
            
        # æ‹¼æ¥ï¼šå®¿ä¸»åœ¨å‰ï¼Œæ”¶å‰²çš„æé€Ÿæºåœ¨å
        # é™åˆ¶å¤–éƒ¨æºæ•°é‡ï¼Œé˜²æ­¢åˆ—è¡¨å¤ªé•¿
        max_add = 60 
        if len(verified_external) > max_add:
            verified_external = verified_external[:max_add]
            
        final_sites = safe_host_sites + verified_external
        base_config['sites'] = final_sites
        
        print(f">>> [5/5] ä¿å­˜...")
        with open("my_tvbox.json", 'w', encoding='utf-8') as f:
            json.dump(base_config, f, ensure_ascii=False, indent=2)
            
        print(f"\nâœ… è¿è¡ŒæˆåŠŸï¼")
        print(f"ğŸ“Š å®¿ä¸»æº: {len(safe_host_sites)} ä¸ª")
        print(f"ğŸš€ æ”¶å‰²æº: {len(verified_external)} ä¸ª (å·²å‰”é™¤æ¯’Jar)")
        print(f"ğŸ›¡ï¸ å½“å‰é˜²å¾¡å¡”(Jar): {base_config['spider']}")
        
    except Exception as e:
        print(f"\n[!!!] é”™è¯¯: {e}")
        if not os.path.exists("my_tvbox.json"):
            with open("my_tvbox.json", 'w', encoding='utf-8') as f:
                json.dump({"spider":"", "sites":[]}, f)
        sys.exit(0)

if __name__ == "__main__":
    main()
