import requests
import json
import re
import concurrent.futures
import os
import sys
from urllib.parse import quote, urljoin

# ================= 1. é…ç½®åŒºåŸŸ =================

MY_GITHUB_TOKEN = "" 
PROXIES = None 

# ã€å®¿ä¸»é…ç½®ã€‘
# æˆ‘ä»¬ä»¥è¿™ä¸ªæºä¸ºåŸºç¡€ï¼Œåªå¾€é‡Œé¢æ·»åŠ ä¸œè¥¿ï¼Œä¸æ”¹åŠ¨å®ƒåŸæœ‰çš„æ ¸å¿ƒ
BASE_URL = "http://www.é¥­å¤ªç¡¬.com/tv"
BASE_HOST = "http://www.é¥­å¤ªç¡¬.com/"

# ã€æœåˆ®åˆ—è¡¨ã€‘(åªä»ä¸­æå–é€šç”¨ CMS/APP æ¥å£)
EXTERNAL_URLS = [
    "http://è‚¥çŒ«.com",
    "http://fty.xxooo.cf/tv",
    "https://æ¯’ç›’.com/tv/",
    "http://æˆ‘ä¸æ˜¯.æ‘¸é±¼å„¿.com",
    "http://ok321.top/tv",
    "http://ok321.top/ok",
    "http://tvbox.ç‹äºŒå°æ”¾ç‰›å¨ƒ.top",
    "https://www.yingm.cc/dm/dm.json",
    "http://home.jundie.top:81/top98.json",
    "http://cdn.qiaoji8.com/tvbox.json",
    "https://cdn.gitmirror.com/bb/xduo/libs/master/index.json",
    "https://gitee.com/free-kingdom/dc/raw/main/T4.json",
    "https://raw.githubusercontent.com/yoursmile66/TVBox/main/XC.json",
    "https://tv.èœå¦®ä¸.top",
    "https://api.hgyx.vip/hgyx.json",
    "https://dxawi.github.io/0/0.json",
    "http://www.mitvbox.xyz/å°ç±³/DEMO.json",
    "http://xhztv.top/xhz",
    "http://xhztv.top/4k.json",
    "https://9877.kstore.space/AnotherD/api.json",
    "https://raw.githubusercontent.com/xyq254245/xyqonlinerule/main/XYQTVBox.json",
    "https://bitbucket.org/xduo/duoapi/raw/master/xpg.json",
    "https://raw.githubusercontent.com/guot55/YGBH/main/vip2.json",
    "http://tv.nxog.top/m/111.php?ou=å…¬ä¼—å·æ¬§æ­Œapp&mz=all&jar=all&b=æ¬§æ­Œ",
    "https://å“ªå’.live/",
    "https://www.252035.xyz/z/FongMi.json",
    "http://www.meowtv.vip/tvbox.json",
    "http://fmys.top/fmys.json",
    "https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json",
    "https://gitee.com/yiwu369/6758/raw/master/%E9%9D%92%E9%BE%99/1.json",
    "https://raw.githubusercontent.com/maoystv/6/main/000.json",
    "https://cnb.cool/fish2018/duanju/-/git/raw/main/tvbox.json",
    "https://raw.githubusercontent.com/chitue/dongliTV/main/api.json",
    "https://cnb.cool/aooooowuuuuu/FreeSpider/-/git/raw/main/config",
    "https://android.lushunming.qzz.io/json/index.json",
    "https://www.iyouhun.com/tv/dc",
    "https://www.iyouhun.com/tv/yh",
    "https://9877.kstore.space/AnotherDS/api.json",
    "http://xhztv.top/dc/",
    "http://xhztv.top/DC.txt",
    "https://bitbucket.org/xduo/cool/raw/main/room.json",
    "https://qixing.myhkw.com/DC.txt",
    "http://xmbjm.fh4u.org/dc.txt"
]

# ã€è¿‡æ»¤é…ç½®ã€‘
# ä¸¥ç¦å¼•å…¥ Spider(Type 3)ï¼Œå› ä¸ºä¼šå’Œé¥­å¤ªç¡¬çš„ Jar å†²çªå¯¼è‡´é—ªé€€
ALLOWED_TYPES = [0, 1, 4] 
BLACKLIST = ["å¤±æ•ˆ", "æµ‹è¯•", "å¹¿å‘Š", "æ”¶è´¹", "ç¾¤", "åŠ V", "æŒ‚å£", "Qç¾¤", "ä¼¦ç†", "ç¦åˆ©", "æˆäºº", "æƒ…è‰²", "å¼•æµ", "å¼¹å¹•", "æ›´æ–°", "é¥­å¤ªç¡¬"] # é¥­å¤ªç¡¬è‡ªå·±ä¸ç”¨é‡å¤åŠ 
TIMEOUT = 5
MAX_WORKERS = 20

# ================= 2. å·¥å…·å‡½æ•° =================

def decode_content(content):
    if not content: return None
    try:
        return json.loads(content)
    except:
        pass
    try:
        # å¤„ç†ç®€å•çš„ Base64 æˆ– å¹²æ‰°å­—ç¬¦
        clean = content.replace('**', '').replace('o', '').strip()
        return json.loads(base64.b64decode(clean).decode('utf-8'))
    except:
        try:
            match = re.search(r'\{.*\}', content, re.DOTALL)
            if match: return json.loads(match.group())
        except: pass
    return None

def get_json(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, timeout=8, verify=False, proxies=PROXIES)
        res.encoding = 'utf-8'
        if res.status_code == 200:
            return decode_content(res.text)
    except: pass
    return None

def clean_name(name):
    return re.sub(r'ã€.*?ã€‘|\[.*?\]|\(.*?\)', '', str(name)).replace("èšåˆ", "").replace("è“å…‰", "").strip()

# ================= 3. æ ¸å¿ƒé€»è¾‘ =================

def fetch_base_config():
    """è·å–é¥­å¤ªç¡¬åŸå§‹é…ç½®ï¼Œå¹¶ä¿®å¤è·¯å¾„"""
    print(f">>> [1/5] æ­£åœ¨æ‹‰å–å®¿ä¸»é…ç½® (é¥­å¤ªç¡¬): {BASE_URL} ...")
    base = get_json(BASE_URL)
    if not base:
        print("!!! æ— æ³•è·å–é¥­å¤ªç¡¬é…ç½®ï¼Œè„šæœ¬ç»ˆæ­¢ã€‚")
        sys.exit(1)
    
    # ä¿®å¤ Spider Jar è·¯å¾„ (è½¬ä¸ºç»å¯¹è·¯å¾„)
    if 'spider' in base:
        spider = base['spider']
        if spider.startswith('./'):
            base['spider'] = urljoin(BASE_HOST, spider)
            print(f"    [âˆš] ä¿®å¤ Jar è·¯å¾„: {base['spider']}")
    
    # ä¿®å¤ Wallpaper è·¯å¾„
    if 'wallpaper' in base:
        wp = base['wallpaper']
        if wp.startswith('./'):
            base['wallpaper'] = urljoin(BASE_HOST, wp)

    return base

def fetch_external_candidates():
    """è·å–å¤–éƒ¨æ‰€æœ‰æºåˆ—è¡¨"""
    print(f">>> [2/5] æ­£åœ¨æœåˆ®å¤–éƒ¨å€™é€‰æº...")
    all_urls = EXTERNAL_URLS.copy()
    
    # ç®€å•çš„å®˜ç½‘æŠ“å–
    try:
        res = requests.get(BASE_HOST, timeout=10, verify=False, proxies=PROXIES)
        matches = re.findall(r'(https?://[^\s"<>]+)', res.text)
        for u in matches:
            if '.json' in u and u not in all_urls: all_urls.append(u)
    except: pass

    # å±•å¼€å¤šä»“
    candidates_sites = []
    
    def process_url(url):
        data = get_json(url)
        if not data: return []
        
        # å¦‚æœæ˜¯å¤šä»“ï¼Œæå–å­é“¾æ¥
        if 'urls' in data and isinstance(data['urls'], list):
            sub_sites = []
            for item in data['urls']:
                if 'url' in item:
                    sub_data = get_json(item['url'])
                    if sub_data and 'sites' in sub_data:
                        sub_sites.extend(sub_data['sites'])
            return sub_sites
            
        # å¦‚æœæ˜¯å•ä»“ï¼Œæå– sites
        if 'sites' in data:
            return data['sites']
        return []

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(process_url, url): url for url in all_urls}
        for future in concurrent.futures.as_completed(futures):
            res = future.result()
            if res: candidates_sites.extend(res)
            
    print(f"    [+] æœé›†åˆ° {len(candidates_sites)} ä¸ªæ½œåœ¨æ¥å£")
    return candidates_sites

def validate_and_filter(sites):
    """ç­›é€‰ï¼šåªç•™é€šç”¨æ¥å£ï¼Œä¸”å¿…é¡»èƒ½è¿é€š"""
    print(f">>> [3/5] æ­£åœ¨è¿›è¡Œå…¼å®¹æ€§ç­›é€‰ä¸æµ‹é€Ÿ...")
    
    valid_sites = []
    seen_api = set()
    
    # é¢„å¤„ç†ï¼šå…ˆå»é‡ï¼Œä¸”åªä¿ç•™ Type 0/1/4
    tasks = []
    for s in sites:
        name = s.get('name', '')
        api = s.get('api', '')
        stype = s.get('type', 0)
        
        # 1. ç±»å‹è¿‡æ»¤ (æ‹’ç» Type 3 Spiderï¼Œé˜²æ­¢å†²çª)
        if stype not in ALLOWED_TYPES: continue
        
        # 2. å…³é”®è¯è¿‡æ»¤
        if any(bw in name for bw in BLACKLIST): continue
        
        # 3. å»é‡
        if api in seen_api: continue
        seen_api.add(api)
        
        tasks.append(s)

    # å¹¶å‘æµ‹é€Ÿ
    def check(site):
        try:
            # æ·±åº¦æ£€æµ‹ï¼šå°è¯•è·å– JSON
            res = requests.get(site['api'], timeout=TIMEOUT, verify=False, proxies=PROXIES)
            if res.status_code == 200:
                # ç®€å•éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆ JSON (é˜²æ­¢ HTML ä¼ªè£…)
                if res.text.strip().startswith('{') or res.text.strip().startswith('['):
                    latency = int(res.elapsed.total_seconds() * 1000)
                    site['_latency'] = latency
                    site['name'] = f"ğŸš€ {clean_name(site['name'])}"
                    return site
        except: pass
        return None

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(check, s) for s in tasks]
        for future in concurrent.futures.as_completed(futures):
            res = future.result()
            if res: valid_sites.append(res)
            
    # æŒ‰é€Ÿåº¦æ’åº
    valid_sites.sort(key=lambda x: x['_latency'])
    # æ¸…ç†ä¸´æ—¶å­—æ®µ
    for s in valid_sites: s.pop('_latency', None)
    
    print(f"    [âˆš] ç­›é€‰å‡º {len(valid_sites)} ä¸ªä¼˜è´¨é€šç”¨æº")
    return valid_sites

def main():
    requests.packages.urllib3.disable_warnings()
    print(">>> å¯åŠ¨ TVBox å¯„ç”Ÿæ¨¡å¼ä¼˜åŒ–è„šæœ¬ v15.0")
    
    # 1. è·å–å®¿ä¸» (é¥­å¤ªç¡¬)
    base_config = fetch_base_config()
    
    # 2. è·å–å¹¶æ¸…æ´—å¤–éƒ¨æº
    raw_external = fetch_external_candidates()
    verified_external = validate_and_filter(raw_external)
    
    # 3. èåˆ (Grafting)
    print(f">>> [4/5] æ­£åœ¨è¿›è¡Œé…ç½®èåˆ...")
    
    # ä¿ç•™å®¿ä¸»åŸæœ¬çš„æ‰€æœ‰ siteï¼Œä½†ç»™å®ƒä»¬åŠ ä¸Šæ ‡è®°
    host_sites = base_config.get('sites', [])
    for s in host_sites:
        # ç»™é¥­å¤ªç¡¬åŸç‰ˆåŠ ä¸ªæ˜Ÿæ˜Ÿï¼Œæ’åœ¨æœ€å‰
        if "é¥­å¤ªç¡¬" not in s['name']:
            s['name'] = f"â˜… {s['name']}"
            
    # å°†å¤–éƒ¨ä¼˜è´¨æºè¿½åŠ åˆ°åé¢
    # æˆªå–å‰ 50 ä¸ªæœ€å¿«çš„å¤–éƒ¨æºï¼Œé˜²æ­¢åˆ—è¡¨è¿‡é•¿å¯¼è‡´å†…å­˜æº¢å‡º
    max_add = 50
    if len(verified_external) > max_add:
        verified_external = verified_external[:max_add]
        
    final_sites = host_sites + verified_external
    base_config['sites'] = final_sites
    
    # 4. ä¿å­˜
    print(f">>> [5/5] ä¿å­˜é…ç½®...")
    with open("my_tvbox.json", 'w', encoding='utf-8') as f:
        json.dump(base_config, f, ensure_ascii=False, indent=2)
        
    print(f"\nâœ… æˆåŠŸï¼")
    print(f"ğŸ“Š å®¿ä¸»æº: {len(host_sites)} ä¸ª")
    print(f"ğŸš€ æŒ‚è½½æº: {len(verified_external)} ä¸ª (å·²å‰”é™¤ä¸å…¼å®¹çš„Jaræº)")
    print(f"ğŸ“‚ æ ¸å¿ƒ Jar: {base_config.get('spider')}")

if __name__ == "__main__":
    main()
