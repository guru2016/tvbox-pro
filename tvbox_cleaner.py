import json
import requests
from copy import deepcopy

# ================= 1. é…ç½®åŒºåŸŸ =================

# é¥­å¤ªç¡¬å®˜æ–¹åº•æ¿
BASE_URL = "http://www.é¥­å¤ªç¡¬.com/tv"

# è¿½åŠ å¯è§£ææº
EXTRA_SOURCES = [
    "https://raw.githubusercontent.com/yoursmile66/TVBox/main/XC.json",
    "https://raw.githubusercontent.com/fantaite/TVBox/main/XC.json",
]

TIMEOUT = 10


# ================= 2. å·¥å…·å‡½æ•° =================

def fetch_json(url):
    """å®‰å…¨è·å– JSONï¼Œç©ºæˆ–é JSON è‡ªåŠ¨è·³è¿‡"""
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers, timeout=TIMEOUT)
        r.raise_for_status()
        if not r.text.strip():
            print(f"[è·³è¿‡] ç©ºå“åº”: {url}")
            return None
        return r.json()
    except Exception as e:
        print(f"[è·³è¿‡] æ— æ³•è·å–: {url} -> {e}")
        return None


def normalize_site(site):
    """æ ¼å¼æ ¡éªŒä¸æœç´¢å­—æ®µä¿®å¤"""
    if not isinstance(site, dict):
        return None

    required = ["key", "name", "api", "type"]
    if not all(k in site for k in required):
        return None

    s = deepcopy(site)

    # è¡¥å……æœç´¢å­—æ®µï¼Œé¿å…æœç´¢å¤±æ•ˆ
    s.setdefault("searchable", 1)
    s.setdefault("quickSearch", 1)

    # type è½¬æ•´å‹
    try:
        s["type"] = int(s["type"])
    except Exception:
        return None

    # api å¿…é¡»æ˜¯ HTTP / HTTPS
    if not isinstance(s["api"], str) or not s["api"].startswith("http"):
        return None

    return s


# ================= 3. æ ¸å¿ƒé€»è¾‘ =================

def main():
    print(">>> æ‹‰å–é¥­å¤ªç¡¬ä¸»é…ç½®...")
    base = fetch_json(BASE_URL)
    if not base:
        print("[é”™è¯¯] é¥­å¤ªç¡¬æºä¸å¯ç”¨ï¼Œé€€å‡º")
        return

    result = deepcopy(base)

    # å»ºç«‹å·²æœ‰ key é›†åˆï¼Œç”¨äºå»é‡
    base_sites = {s["key"] for s in result.get("sites", []) if "key" in s}
    merged_sites = []

    print(f"é¥­å¤ªç¡¬åŸå§‹ç«™ç‚¹æ•°: {len(base_sites)}")

    # å¤„ç†é™„åŠ æº
    for src in EXTRA_SOURCES:
        print(f"å¤„ç†é™„åŠ æº: {src}")
        data = fetch_json(src)
        if not data or "sites" not in data:
            continue

        for site in data["sites"]:
            s = normalize_site(site)
            if not s:
                continue
            # ä¸è¦†ç›–é¥­å¤ªç¡¬åŸå§‹ç«™ç‚¹
            if s["key"] in base_sites:
                continue
            merged_sites.append(s)
            base_sites.add(s["key"])

    print(f"æˆåŠŸåˆå¹¶æ–°å¢ç«™ç‚¹: {len(merged_sites)}")

    # åˆå¹¶æœ€ç»ˆç»“æœ
    result["sites"].extend(merged_sites)

    # ç¡®ä¿ä¸€äº›å¿…éœ€å­—æ®µå­˜åœ¨
    result.setdefault("lives", [])
    result.setdefault("parses", [])
    result.setdefault("rules", [])

    # ä¿å­˜è¾“å‡ºæ–‡ä»¶
    out_file = "tvbox_fty_merged.json"
    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"âœ… å®Œæˆç”Ÿæˆ: {out_file}")
    print(f"ğŸ“Š æœ€ç»ˆç«™ç‚¹æ€»æ•°: {len(result['sites'])}")


if __name__ == "__main__":
    main()