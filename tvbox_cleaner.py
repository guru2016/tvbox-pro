import requests
import json
import re
import concurrent.futures
import os
import sys
from urllib.parse import quote, urljoin

# ================= 1. é…ç½®åŒºåŸŸ =================

MY_GITHUB_TOKEN = "" 
PROXIES = None 

# ã€ä¸ªäººåŸºç¡€é…ç½®ã€‘
GITHUB_USER = "guru2016"
REPO_NAME = "tvbox-pro"
BRANCH_NAME = "main"
# å…¨å±€ä¿åº• Jar (å½“æ‰¾ä¸åˆ°ç‰¹å®š Jar æ—¶ç”¨è¿™ä¸ª)
GLOBAL_SAFE_JAR = f"https://cdn.jsdelivr.net/gh/{GITHUB_USER}/{REPO_NAME}@{BRANCH_NAME}/spider.jar"

# ã€æ™ºèƒ½ Jar åŒ¹é…è¡¨ã€‘(æ ¸å¿ƒä¿®æ”¹)
# æ ¼å¼: "APIå…³é”®è¯": "å¯¹åº”çš„ä¸“ç”¨Jaråœ°å€"
# é‡åˆ°è¿™äº› API æ—¶ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨ç»™å®ƒåˆ†é…ä¸“å± Jarï¼Œç¡®ä¿èƒ½ç”¨ä¸”æ— å¹¿å‘Š
SPECIFIC_JARS = {
    "csp_Xpg": "https://cdn.jsdelivr.net/gh/yoursmile66/TVBox@main/jar/xpg.jar", # å°è‹¹æœä¸“ç”¨
    "csp_Wogg": "https://cdn.jsdelivr.net/gh/yoursmile66/TVBox@main/jar/wogg.jar", # ç©å¶ä¸“ç”¨
    "csp_Nmys": "https://cdn.jsdelivr.net/gh/yoursmile66/TVBox@main/jar/nmys.jar", # å†œæ°‘/ç³¯ç±³ä¸“ç”¨
    "csp_Panda": "https://cdn.jsdelivr.net/gh/yoursmile66/TVBox@main/jar/panda.jar",
    "csp_Jianpian": "https://cdn.jsdelivr.net/gh/2hacc/TVBox@main/jar/jp.jar" # èç‰‡ä¸“ç”¨
}

# ã€Jar åŒ…åŸŸåç™½åå•ã€‘
# åªæœ‰è¿™äº›åŸŸåä¸‹çš„ Jar å…è®¸è¢«ä¿ç•™ï¼Œå…¶ä»–çš„æ‚ç‰Œ Jar ä¸€å¾‹æ›¿æ¢ä¸º GLOBAL_SAFE_JAR
SAFE_JAR_DOMAINS = [
    "cdn.jsdelivr.net",
    "raw.githubusercontent.com",
    "ghproxy.com",
    "ghproxy.net",
    "fastly.jsdelivr.net",
    "cdn.qiaoji8.com", # å·§æŠ€çš„JaræŠ€æœ¯ä¸Šæ˜¯æ²¡æ¯’çš„ï¼Œé…ç½®æœ‰æ¯’
    "gitlab.com"
]

# ã€å®¿ä¸»åˆ—è¡¨ã€‘
HOST_URLS = [
    "http://www.é¥­å¤ªç¡¬.com/tv",       
    "http://è‚¥çŒ«.com",
    "http://fty.xxooo.cf/tv"
]

# ã€æœåˆ®åˆ—è¡¨ã€‘
EXTERNAL_URLS = [
    # èšåˆå¤§å‚
    "http://ok321.top/tv",
    "http://tvbox.ç‹äºŒå°æ”¾ç‰›å¨ƒ.top",
    "https://tv.èœå¦®ä¸.top",
    "https://api.hgyx.vip/hgyx.json",
    "https://raw.githubusercontent.com/yoursmile66/TVBox/main/XC.json",
    "https://raw.githubusercontent.com/guot55/YGBH/main/vip2.json",
    "https://cnb.cool/fish2018/duanju/-/git/raw/main/tvbox.json",
    "https://raw.githubusercontent.com/chitue/dongliTV/main/api.json",
    "https://android.lushunming.qzz.io/json/index.json",
    "http://home.jundie.top:81/top98.json",
    "https://cdn.gitmirror.com/bb/xduo/libs/master/index.json",
    # ä¼˜è´¨å•ä»“
    "https://ghproxy.net/https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json",
    "https://www.252035.xyz/z/FongMi.json"
]

# ã€è¿‡æ»¤é…ç½®ã€‘
ALLOWED_TYPES = [0, 1, 3, 4] # å…¨é¢æ”¾å¼€ Type 3
BLACKLIST = [
    "å¤±æ•ˆ", "æµ‹è¯•", "å¹¿å‘Š", "æ”¶è´¹", "ç¾¤", "åŠ V", "æŒ‚å£", "Qç¾¤", "ä¼¦ç†", "ç¦åˆ©", "æˆäºº", "æƒ…è‰²", 
    "å¼•æµ", "å¼¹å¹•", "æ›´æ–°", "å…¬ä¼—å·", "æ‰«ç ", "å¾®ä¿¡", "ä¼é¹…", "APP", "ä¸‹è½½", 
    "æ¨å¹¿", "éªŒè¯", "æ¿€æ´»", "æˆæƒ", "é›·é²¸", "ç©å¶å“¥å“¥", "åŠ©æ‰‹", "ä¸“çº¿", "å½©è›‹", "ç›´æ’­"
]

TIMEOUT = 8 
MAX_WORKERS = 20

# ================= 2. å·¥å…·å‡½æ•° =================

def decode_content(content):
    if not content: return None
    try: return json.loads(content)
    except: pass
    try:
        clean = content.replace('**', '').replace('o', '').strip()
        return json.loads(base64.b64decode(clean).decode('utf-8'))
    except:
        try:
            match = re.search(r'\{.*\}', content, re.DOTALL)
            if match: return json.loads(match.group())
        except: pass
    return None

def get_json(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, timeout=TIMEOUT, verify=False, proxies=PROXIES)
        res.encoding = 'utf-8'
        if res.status_code == 200:
            return decode_content(res.text)
    except: pass
    return None

def clean_name(name):
    name = str(name)
    name = re.sub(r'ã€.*?ã€‘|\[.*?\]|\(.*?\)|ï¼ˆ.*?ï¼‰', '', name)
    name = name.replace("èšåˆ", "").replace("è“å…‰", "").replace("ä¸“çº¿", "").strip()
    return name

# ================= 3. æ ¸å¿ƒï¼šJar æ™ºèƒ½å¤„ç† =================

def process_site_jar(site):
    """
    æ™ºèƒ½åˆ†é… Jar åŒ…ï¼š
    1. å¦‚æœ API åœ¨ SPECIFIC_JARS é‡Œï¼Œå¼ºåˆ¶èµ‹äºˆä¸“ç”¨ Jarã€‚
    2. å¦‚æœæºè‡ªå¸¦ Jarï¼Œæ£€æŸ¥åŸŸåæ˜¯å¦åœ¨ç™½åå•ã€‚
       - åœ¨ç™½åå•ï¼šä¿ç•™ã€‚
       - ä¸åœ¨ç™½åå•ï¼šå‰”é™¤ Jar (ä½¿ç”¨å…¨å±€ Jar)ã€‚
    3. å¦‚æœæ²¡æœ‰ Jarï¼Œä¿æŒåŸæ · (ä½¿ç”¨å…¨å±€ Jar)ã€‚
    """
    api = site.get('api', '')
    original_jar = site.get('jar', '')
    
    # ç­–ç•¥ 1: ç²¾å‡†åŒ¹é… (ä¼˜å…ˆçº§æœ€é«˜)
    for key, specific_jar in SPECIFIC_JARS.items():
        if key in api:
            site['jar'] = specific_jar
            # print(f"    [ğŸ”§] è‡ªåŠ¨é€‚é… Jar: {site['name']} -> {specific_jar}")
            return site

    # ç­–ç•¥ 2: ç°æœ‰ Jar å®‰å…¨æ€§æ£€æŸ¥
    if original_jar:
        is_safe = False
        for domain in SAFE_JAR_DOMAINS:
            if domain in original_jar:
                is_safe = True
                break
        
        if not is_safe:
            # print(f"    [ğŸ›¡ï¸] æ‹¦æˆªå¯ç–‘ Jar: {site['name']} -> {original_jar}")
            del site['jar'] # åˆ é™¤æ¯’ Jarï¼Œå›é€€åˆ°å…¨å±€ Jar
    
    return site

# ================= 4. æµç¨‹å‡½æ•° =================

def fetch_base_config_fail_safe():
    print(f">>> [1/5] è¿æ¥å®¿ä¸»...")
    if "guru2016" not in GLOBAL_SAFE_JAR:
        print("!!! è­¦å‘Šï¼šGITHUB_USER æœªé…ç½®ï¼")

    for url in HOST_URLS:
        print(f"    - å°è¯•: {url}")
        base = get_json(url)
        if base and isinstance(base, dict) and 'sites' in base:
            print(f"    [âˆš] é”å®šå®¿ä¸»: {url}")
            base_host = url.rsplit('/', 1)[0] + '/'
            
            # å®¿ä¸»é»˜è®¤ä½¿ç”¨å…¨å±€å®‰å…¨ Jar
            base['spider'] = GLOBAL_SAFE_JAR
            
            if 'wallpaper' in base and isinstance(base['wallpaper'], str) and base['wallpaper'].startswith('./'):
                base['wallpaper'] = urljoin(base_host, base['wallpaper'])
                
            return base
    
    return {
        "spider": GLOBAL_SAFE_JAR,
        "wallpaper": "https://api.kdcc.cn",
        "sites": [], "lives": [], "parses": [], "flags": []
    }

def fetch_external_candidates():
    print(f">>> [2/5] å…¨ç½‘æœåˆ®...")
    all_urls = EXTERNAL_URLS.copy()
    candidates_sites = []
    
    def process_url(url):
        data = get_json(url)
        if not data: return []
        extracted = []
        if 'urls' in data and isinstance(data['urls'], list):
            for item in data['urls']:
                if 'url' in item:
                    sub_data = get_json(item['url'])
                    if sub_data and 'sites' in sub_data:
                        extracted.extend(sub_data['sites'])
        if 'sites' in data:
            extracted.extend(data['sites'])
        return extracted

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(process_url, url): url for url in all_urls}
        for future in concurrent.futures.as_completed(futures):
            res = future.result()
            if res: candidates_sites.extend(res)
            
    print(f"    [+] æœé›†åˆ° {len(candidates_sites)} ä¸ªæ¥å£")
    return candidates_sites

def validate_and_filter(sites):
    print(f">>> [3/5] æ™ºèƒ½æ¸…æ´— & åŒ¹é…Jar...")
    
    valid_sites = []
    seen_api = set()
    tasks = []
    
    for s in sites:
        name = s.get('name', '')
        api = s.get('api', '')
        stype = s.get('type', 0)
        
        if stype not in ALLOWED_TYPES: continue
        if any(bw in name for bw in BLACKLIST): continue
        if api in seen_api: continue
        if any(char in name for char in ['ğŸ’°', 'ğŸ‘—', 'ğŸ‘ ', 'âœ¨', 'âš¡', 'ğŸ”¥', 'å…è´¹', 'é€', 'åŠ V']): continue

        # ã€æ ¸å¿ƒæ­¥éª¤ã€‘å¤„ç† Jar åŒ…
        s = process_site_jar(s)

        seen_api.add(api)
        tasks.append(s)

    def check(site):
        try:
            # åªè¦èƒ½é€šå°±è¡Œï¼Œä¸å¼ºæ±‚ JSONï¼Œå› ä¸ºæœ‰äº› Type 3 æ˜¯åŠ å¯†æ•°æ®
            res = requests.get(site['api'], timeout=TIMEOUT, verify=False, proxies=PROXIES)
            if res.status_code == 200 and len(res.content) > 10:
                latency = int(res.elapsed.total_seconds() * 1000)
                site['_latency'] = latency
                site['name'] = clean_name(site['name'])
                
                # å¼€å¯æœç´¢
                site['searchable'] = 1 
                site['quickSearch'] = 1
                
                # æ ‡è®°ï¼šå¦‚æœè¿™ä¸ªæºæœ‰ç‹¬ç«‹ Jarï¼Œç»™ä¸ªç‰¹æ®Šå›¾æ ‡
                if 'jar' in site:
                    site['name'] = f"ğŸ§© {site['name']}"
                else:
                    site['name'] = f"ğŸš€ {site['name']}"
                    
                return site
        except: pass
        return None

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(check, s) for s in tasks]
        for future in concurrent.futures.as_completed(futures):
            res = future.result()
            if res: valid_sites.append(res)
            
    valid_sites.sort(key=lambda x: x['_latency'])
    for s in valid_sites: s.pop('_latency', None)
    
    print(f"    [âˆš] æ¸…æ´—å®Œæ¯•ï¼Œå‰©ä½™ {len(valid_sites)} ä¸ªæœ‰æ•ˆæº")
    return valid_sites

def main():
    try:
        requests.packages.urllib3.disable_warnings()
        print(">>> å¯åŠ¨ TVBox æ™ºèƒ½å¤šJarç‰ˆ v20.0")
        
        base_config = fetch_base_config_fail_safe()
        
        raw_external = fetch_external_candidates()
        verified_external = validate_and_filter(raw_external)
        
        print(f">>> [4/5] èåˆé…ç½®...")
        host_sites = base_config.get('sites', [])
        
        safe_host_sites = []
        for s in host_sites:
            s['name'] = f"â˜… {clean_name(s['name'])}"
            s['searchable'] = 1
            # å®¿ä¸»é‡Œçš„æºä¹Ÿéœ€è¦è¿‡ä¸€é Jar æ£€æŸ¥ï¼Œé˜²æ­¢å®¿ä¸»è‡ªå¸¦æ¯’
            s = process_site_jar(s)
            safe_host_sites.append(s)
            
        max_add = 100 # å¢åŠ åˆ° 100 ä¸ªï¼Œå› ä¸ºç°åœ¨æœ‰å¾ˆå¤š Type 3 äº†
        if len(verified_external) > max_add:
            verified_external = verified_external[:max_add]
            
        final_sites = safe_host_sites + verified_external
        base_config['sites'] = final_sites
        
        print(f">>> [5/5] ä¿å­˜...")
        with open("my_tvbox.json", 'w', encoding='utf-8') as f:
            json.dump(base_config, f, ensure_ascii=False, indent=2)
            
        print(f"\nâœ… æˆåŠŸï¼")
        print(f"ğŸ“Š æ€»è®¡æº: {len(final_sites)} ä¸ª")
        print(f"ğŸ§© ç‹¬ç«‹Jaræº: {len([s for s in final_sites if 'ğŸ§©' in s['name']])} ä¸ª")
        
    except Exception as e:
        print(f"\n[!!!] é”™è¯¯: {e}")
        if not os.path.exists("my_tvbox.json"):
            with open("my_tvbox.json", 'w', encoding='utf-8') as f:
                json.dump({"spider":"", "sites":[]}, f)
        sys.exit(0)

if __name__ == "__main__":
    main()
