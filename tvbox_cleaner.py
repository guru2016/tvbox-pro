import requests
import json
import re
import concurrent.futures
import os
import sys
import base64
from urllib.parse import quote, urljoin

# ================= é…ç½®åŒºåŸŸ =================
MY_GITHUB_TOKEN = ""
PROXIES = None

GITHUB_USER = "guru2016"
REPO_NAME = "tvbox-pro"
BRANCH_NAME = "main"

GLOBAL_SAFE_JAR = f"https://cdn.jsdelivr.net/gh/{GITHUB_USER}/{REPO_NAME}@{BRANCH_NAME}/spider.jar"

EXTERNAL_URLS = [
    "http://www.é¥­å¤ªç¡¬.com/tv",
    "http://è‚¥çŒ«.com",
    "http://fty.xxooo.cf/tv",
    "http://rihou.cc:88/è·åŸèŒ¶ç§€",
    "https://api.hgyx.vip/hgyx.json",
    "https://tv.èœå¦®ä¸.top",
    "https://raw.githubusercontent.com/yoursmile66/TVBox/main/XC.json",
    "https://raw.githubusercontent.com/guot55/YGBH/main/vip2.json",
    "https://cnb.cool/fish2018/duanju/-/git/raw/main/tvbox.json",
    "https://raw.githubusercontent.com/chitue/dongliTV/main/api.json",
    "https://cdn.gitmirror.com/bb/xduo/libs/master/index.json",
    "http://ok321.top/tv",
    "http://tvbox.ç‹äºŒå°æ”¾ç‰›å¨ƒ.top",
    "https://android.lushunming.qzz.io/json/index.json",
    "http://home.jundie.top:81/top98.json",
    "https://ghproxy.net/https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json",
    "https://www.252035.xyz/z/FongMi.json",
    "http://52bsj.vip:81/api/v3/file/get/29899/bsj2023.json?sign=3c594b2b985b365bad",
    "https://s2.pub/x",
    "http://tv.nxog.top/m/111.php?ou=å…¬ä¼—å·æ¬§æ­Œapp&mz=all&jar=all&b=æ¬§æ­Œ",
    "https://100km.top/0",
    "http://meowtv.cn/tv",
    "http://cdn.qiaoji8.com/tvbox.json"
]

ALLOWED_TYPES = [0, 1, 3, 4]
BLACKLIST = [
    "å¤±æ•ˆ", "æµ‹è¯•", "å¹¿å‘Š", "æ”¶è´¹", "ç¾¤", "åŠ V", "æŒ‚å£", "Qç¾¤", "ä¼¦ç†", "ç¦åˆ©", "æˆäºº", "æƒ…è‰²",
    "å¼•æµ", "å¼¹å¹•", "æ›´æ–°", "å…¬ä¼—å·", "æ‰«ç ", "å¾®ä¿¡", "ä¼é¹…", "APP", "ä¸‹è½½",
    "æ¨å¹¿", "éªŒè¯", "æ¿€æ´»", "æˆæƒ", "é›·é²¸", "ç©å¶å“¥å“¥", "åŠ©æ‰‹", "ä¸“çº¿", "å½©è›‹", "ç›´æ’­",
    "77.110", "mingming"
]

TIMEOUT = 6
MAX_WORKERS = 60
OUTPUT_FILE = "my_tvbox.json"

# ================= å·¥å…·å‡½æ•° =================
def decode_content(content):
    if not content:
        return None
    try:
        return json.loads(content)
    except:
        pass
    try:
        clean = content.replace('**', '').replace('o', '').strip()
        return json.loads(base64.b64decode(clean).decode('utf-8'))
    except:
        try:
            match = re.search(r'\{.*\}', content, re.DOTALL)
            if match:
                return json.loads(match.group())
        except:
            pass
    return None

def get_json(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        res = requests.get(url, headers=headers, timeout=TIMEOUT, verify=False, proxies=PROXIES)
        res.encoding = 'utf-8'
        if res.status_code == 200:
            return decode_content(res.text)
    except:
        pass
    return None

def clean_name(name):
    name = str(name)
    name = re.sub(r'ã€.*?ã€‘|\[.*?\]|\(.*?\)|ï¼ˆ.*?ï¼‰', '', name)
    name = name.replace("èšåˆ", "").replace("è“å…‰", "").replace("ä¸“çº¿", "").strip()
    return name

def fetch_and_strip(url):
    data = get_json(url)
    if not data:
        return []
    extracted_sites = []

    def process_site(s):
        if 'jar' in s:
            del s['jar']
        return s

    if 'urls' in data and isinstance(data['urls'], list):
        for item in data['urls']:
            if 'url' in item:
                sub_data = get_json(item['url'])
                if sub_data and 'sites' in sub_data:
                    for s in sub_data['sites']:
                        extracted_sites.append(process_site(s))
    if 'sites' in data:
        for s in data['sites']:
            extracted_sites.append(process_site(s))
    return extracted_sites

def fetch_all_sites_stripped():
    all_sites = []
    unique_urls = list(set(EXTERNAL_URLS))
    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
        future_to_url = {executor.submit(fetch_and_strip, url): url for url in unique_urls}
        for future in concurrent.futures.as_completed(future_to_url):
            try:
                sites = future.result()
                if sites:
                    all_sites.extend(sites)
            except:
                pass
    return all_sites

# ================= ã€å…³é”®ã€‘å»æ‰æµ‹é€Ÿï¼Œåªæ¸…æ´— =================
def only_clean_no_speed_test(sites):
    valid_sites = []
    seen_api = set()

    for s in sites:
        name = s.get('name', '')
        api = s.get('api', '')
        stype = s.get('type', 0)

        if stype not in ALLOWED_TYPES:
            continue
        if any(bw in name for bw in BLACKLIST):
            continue
        if api in seen_api:
            continue

        seen_api.add(api)

        # åªæ¸…ç†åå­—ï¼Œä¸å¼€é€šæµ‹é€Ÿ
        s['name'] = clean_name(name)
        s['searchable'] = 1
        s['quickSearch'] = 1

        if s.get('type') == 3:
            s['name'] = f"ğŸ›¡ï¸ {s['name']}"
        else:
            s['name'] = f"ğŸš€ {s['name']}"

        valid_sites.append(s)

    return valid_sites

def main():
    requests.packages.urllib3.disable_warnings()
    print(">>> å¼€å§‹è‡ªåŠ¨æ›´æ–° TVBox é…ç½®ï¼ˆæ— æµ‹é€Ÿï¼Œè¶…å¤šæºï¼‰...")

    raw_sites = fetch_all_sites_stripped()
    final_sites = only_clean_no_speed_test(raw_sites)

    if len(final_sites) > 300:
        final_sites = final_sites[:300]

    lives = [
        {"name": "ğŸ“º å¤®è§†å«è§†", "type": 0, "url": "https://tv.iill.top/m3u/iptv-org.m3u", "ua": "okhttp/3.12.13"},
        {"name": "ğŸ“º é«˜æ¸…ç›´æ’­", "type": 0, "url": "https://raw.githubusercontent.com/sszlxx/IPTV4TVBox/main/live.txt", "ua": "okhttp/3.12.13"}
    ]

    parses = [
        {"name": "âš¡ æé€Ÿè§£æ1", "url": "https://jx.qqmi.cc/jx/player.php?url="},
        {"name": "âš¡ æé€Ÿè§£æ2", "url": "https://jx.777jiexi.com:666/?url="},
        {"name": "âš¡ é€šç”¨è§£æ", "url": "https://www.8090zz.cc/?url="}
    ]

    config = {
        "spider": GLOBAL_SAFE_JAR,
        "wallpaper": "https://api.kdcc.cn",
        "sites": final_sites,
        "lives": lives,
        "parses": parses,
        "flags": []
    }

    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç”Ÿæˆå®Œæˆï¼š{OUTPUT_FILE}ï¼Œæœ‰æ•ˆæºï¼š{len(final_sites)}")
    except Exception as e:
        print(f"âŒ å†™å…¥å¤±è´¥ï¼š{e}")

if __name__ == "__main__":
    main()
